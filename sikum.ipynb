{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "UE7WOPhvkBGf"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOKEgCPyNPGY/cytcXHr1Jc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danben12/course-recape/blob/main/sikum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#class 1\n",
        "https://colab.research.google.com/drive/1DRyfWWe9L6PQDuwRVV751bNX9tj0pSte?authuser=1#scrollTo=BNFw1ovhrRvJ"
      ],
      "metadata": {
        "id": "B32DSLArbQnM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obi-8moJ4LyQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54b68fd6-ddfc-4377-93c4-d29903c2e465"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 3)\n",
            "9\n",
            "float64\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# you can create an array of any dimensions using np.zeros\n",
        "z = np.zeros((3,3))\n",
        "#other handy things\n",
        "identity = np.identity(3)\n",
        "ones = np.ones((2,2))\n",
        "print(identity.shape) #returns tuple of dimensions\n",
        "print(identity.size) # returns total number of elements\n",
        "print(identity.dtype) #the default dtype is float64"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numpy offers several ways to index into arrays. When arrays are one dimensional, indexing works just like lists. When arrays are 2 or more dimensional, you specify an index for each dimension:\n",
        "\n",
        "value = array[row_index, col_index]"
      ],
      "metadata": {
        "id": "BGtMKnWAdQt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#numpy arrays have builtin methods to calculate summary statistics such as std. dev., mean, min, max, sum\n",
        "y = np.random.random(12).reshape(3,4)\n",
        "print(y)\n",
        "print(\"std:\", y.std())\n",
        "print(\"mean:\", y.mean())\n",
        "print(\"sum:\", y.sum())"
      ],
      "metadata": {
        "id": "NZHoFm1_dS7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: What happened here?\n",
        "x = np.array([1,2,3,4]).reshape(2,2)\n",
        "y = np.array([2,2,2,2]).reshape(2,2)\n",
        "\n",
        "print(\"x = \\n\", x)\n",
        "print(\"y = \\n\", y, '\\n')\n",
        "\n",
        "Answer=np.concatenate((x,y), axis = 0)\n",
        "# Answer: concatination over axis 0\n",
        "print(Answer)\n",
        "# TODO: What happened here?\n",
        "\n",
        "result = np.concatenate((x,y), axis = 1)\n",
        "print(\"x = \\n\", x)\n",
        "print(\"y = \\n\", y, '\\n')\n",
        "result\n",
        "\n",
        "# Answer: concatination over axis 1\n",
        "\n",
        "np.transpose(result)\n",
        "\n",
        "#to invert a matrix use np.linalg.inv\n",
        "x = np.array([[1,1], [1,0]])\n",
        "np.linalg.inv(x)"
      ],
      "metadata": {
        "id": "oLAWGNF8eq8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v = np.array([1,2,3])\n",
        "w = np.array([4,5,6])\n",
        "\n",
        "print(\"v = \\n\", v)\n",
        "print(\"w = \\n\", w)\n",
        "print(\"expected dot product = 32\")"
      ],
      "metadata": {
        "id": "KsV4gF52fBAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppose we have a matrix where we want to normalize the rows to have mean zero\n",
        "matrix = 10*np.random.rand(4,5)\n",
        "matrix\n",
        "array = np.array(range(20)).reshape((4,5))\n",
        "output = array > 10\n",
        "output\n",
        "array[output]\n",
        "mask = (array < 5) | (array > 15)\n",
        "mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1TFjtLIftcX",
        "outputId": "fba7f293-de66-46eb-bd8d-eb77436bb032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ True,  True,  True,  True,  True],\n",
              "       [False, False, False, False, False],\n",
              "       [False, False, False, False, False],\n",
              "       [False,  True,  True,  True,  True]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using copy\n",
        "import copy\n",
        "array = np.linspace(1, 10, 10)\n",
        "dup = copy.deepcopy(array) # notice deepcopy (and not just copy)\n",
        "print(id(array))\n",
        "print(id(dup))\n",
        "array[0] = 100\n",
        "dup"
      ],
      "metadata": {
        "id": "xNKkGQtwgL5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importenttt"
      ],
      "metadata": {
        "id": "k0TiwdkMgfxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyparsing.helpers import original_text_for\n",
        "from skimage import io\n",
        "import cv2 \n",
        "\n",
        "def display(img):\n",
        "    # Show image\n",
        "    plt.figure(figsize = (5,5))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('on')\n",
        "    plt.show()\n",
        "    \n",
        "def load(image_path):\n",
        "    out = io.imread(image_path)\n",
        "\n",
        "    # Let's convert the image to be between the correct range.\n",
        "    out = out.astype(np.float64) / 255\n",
        "    return out"
      ],
      "metadata": {
        "id": "dS1I2y27gehk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to exclude a specific channel from a given image\n",
        "def rgb_exclusion(image, channel):\n",
        "    out = image\n",
        "    if channel == 'R':\n",
        "        out[:, :, 0] = 0\n",
        "    elif channel == 'G':\n",
        "        out[:, :, 1] = 0\n",
        "    elif channel == 'B':\n",
        "        out[:, :, 2] = 0\n",
        "\n",
        "    return out\n",
        "\n",
        "no_green = rgb_exclusion(img, 'G')\n",
        "print('No Green')\n",
        "display(no_green)\n",
        "\n",
        "print('Original')\n",
        "\n",
        "display(original) #original\n",
        "\n",
        "# Take a look at both images. Is this what you expected?"
      ],
      "metadata": {
        "id": "7-ZMbCJ_g-Cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ibneZrh1g0xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#class 2\n",
        "https://colab.research.google.com/drive/1Kd52Fa3mmF_YBnhMVQLbriD8Ej8bgqwl?authuser=1"
      ],
      "metadata": {
        "id": "jqV0xFW8hJfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to grayscale and display\n",
        "gray_image = skimage.color.rgb2gray(image)\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.imshow(gray_image, cmap='Greys_r')"
      ],
      "metadata": {
        "id": "hyF-uDeuhOjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the RED channel:\n",
        "red_channel = image[ : , : , 0]\n",
        "# Extracting the GREEN and BLUE channels:\n",
        "green_channel = image[:,:,1]\n",
        "blue_channel = image[:,:,2]\n",
        "# display\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.imshow(red_channel, cmap='Reds_r') # noitce the color map. the '_r' shows the reversed colormap."
      ],
      "metadata": {
        "id": "5s5nyP9DhZZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the channels back again using np.stack()\n",
        "# The axis parameter specifies the index of the new axis in the dimensions of the result. For example, if axis=0 it will be the first dimension and if axis=-1 it will be the last dimension.\n",
        "parrot_stacked = np.stack((red_channel,green_channel,blue_channel),axis=-1) \n",
        "parrot_stacked.shape"
      ],
      "metadata": {
        "id": "ZwB4bkoth5Q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b,g,r = cv2.split(image)\n",
        "merged_image = cv2.merge([r, g, b]) # notice that we merged to RGB and not BGR"
      ],
      "metadata": {
        "id": "VhDsRA-3l07V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##subplot"
      ],
      "metadata": {
        "id": "1moo9qmomTx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,20)) # define figure size\n",
        "\n",
        "im_mona = io.imread(fname=f\"{folder_path}/images/monaLisa.jpg\") #read image using skimage\n",
        "\n",
        "plt.subplot(1, 3, 1) # 1 row, 3 columns, position 1\n",
        "plt.imshow(im_mona)\n",
        "\n",
        "plt.subplot(1, 3, 2) # 1 row, 3 columns, position 2\n",
        "plt.imshow(im_mona[:,:,0],cmap = 'jet')\n",
        "\n",
        "plt.subplot(1, 3, 3) # 1 row, 3 columns, position 3\n",
        "#plt.imshow(im_mona[:,:,0],cmap = 'viridis')\n",
        "plt.imshow(im_mona,cmap = 'jet')"
      ],
      "metadata": {
        "id": "g6esxMzsmW4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_pixel(img,pixel_y,pixel_x,bands):\n",
        "\n",
        "  leaf_pixel = img[\n",
        "      pixel_y:pixel_y+1,\n",
        "      pixel_x:pixel_x+1,\n",
        "      :]\n",
        "\n",
        "  leaf_pixel_squeezed = np.squeeze(leaf_pixel)\n",
        "\n",
        "  plt.plot(bands, leaf_pixel_squeezed)\n",
        "  plt.title('Spectral Footprint\\n(Pixel {},{})'.format(\n",
        "      pixel_x, pixel_y),fontsize=16)\n",
        "  plt.xlabel('Wavelength',fontsize=14)\n",
        "  plt.ylabel('Reflectance',fontsize=14)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "vkkrb2tYmZQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_several_pixel(img,pixels_tuples_array,bands):\n",
        "  for pixel_xy in pixels_tuples_array:\n",
        "    leaf_pixel = img[ \n",
        "      pixel_xy[1]:pixel_xy[1]+1, #y pixel\n",
        "      pixel_xy[1]:pixel_xy[1]+1, #x pixel\n",
        "      :]\n",
        "    leaf_pixel_squeezed = np.squeeze(leaf_pixel) #squeeze\n",
        "\n",
        "    plt.plot(bands, leaf_pixel_squeezed, label =f\"x={pixel_xy[1]}, y={pixel_xy[1]}\"  )\n",
        "  plt.title('Spectral signature',fontsize=16)\n",
        "  plt.xlabel('Wavelength',fontsize=14)\n",
        "  plt.ylabel('Reflectance',fontsize=14)\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "-15eM9hVqKuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " $$NDVI = \\frac{(NIR - RED)}{(NIR + RED)}$$"
      ],
      "metadata": {
        "id": "bf2IoyeIqUG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "red_channel = corrected_nparr[:,:,87] # 651.92 nm\n",
        "nir_channel = corrected_nparr[:,:,140] # 810.86 nm\n",
        "\n",
        "NDVI = (nir_channel - red_channel) / (nir_channel + red_channel) # calc manually\n",
        "vi = ndvi(corrected_nparr, 87, 140) # using the ndvi function from spectral to calc, arguments: image, red band number, nir band number"
      ],
      "metadata": {
        "id": "QOWPaht7qVal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import skimage.io as io\n",
        "# load file paths from chosen folder into a list\n",
        "path_list = glob.glob(f'{folder_path}/images/*.*') #Rerurns a list of file names\n",
        "print(path_list)  #Prints the list containing file names\n",
        "#Now let us load each file at a time...\n",
        "image_list=[]  # Empty list to store images from the folder.\n",
        "path = \"images/test_images/*.*\"\n",
        "for path in path_list:   #Iterate through each file in the list using for\n",
        "    print(path)     # just stop here to see all file names printed\n",
        "    img = io.imread(path)  # now, we can read each file since we have the full path\n",
        "    image_list.append(img)  #C reate a list of images (not just file names but full images)"
      ],
      "metadata": {
        "id": "nKcVAyQVqXxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,15)) # define figure size\n",
        "image_list=[]  # Empty list to store images from the folder.\n",
        "path = \"images/test_images/*.*\"\n",
        "for path in path_list:   #Iterate through each file in the list using for\n",
        "    img = io.imread(path)  # now, we can read each file since we have the full path\n",
        "    image_list.append(img)  #Create a list of images (not just file names but full images)\n",
        "i=0\n",
        "while i<len(image_list):\n",
        "  plt.subplot(1,len(image_list) ,i+1 ) # 2 rows, 2 columns, position 1\n",
        "  mg=image_list[i]\n",
        "  plt.imshow(mg[:,:,0])\n",
        "  i+=1"
      ],
      "metadata": {
        "id": "Z4sOue_Dqs59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#class 3\n",
        "https://colab.research.google.com/drive/1ibgEQ0dleRpwLq_6i0Zba70Gg1f5umxg?authuser=1#scrollTo=IfMcLz-svgBo"
      ],
      "metadata": {
        "id": "qWPW8xoDrkiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to create a folder if doesn't exist\n",
        "\n",
        "import os # import os lib\n",
        "\n",
        "def createDir(path):\n",
        "  doesExist = os.path.exists(path) # checks whether the specified path exists\n",
        "  if not doesExist:\n",
        "    os.makedirs(path) # create path, since it doesn't exist\n",
        "    print(\"The new directory was created!\")\n",
        "    # create images folder in our folder_path\n",
        "images_path = f'{folder_path}/images' # your folder you want to create!\n",
        "createDir(images_path)\n",
        "# lets check that the images folder was actually created\n",
        "if os.path.exists(images_path):\n",
        "  print(f'Path: {images_path}\\nThe above path exists!')\n",
        "else:\n",
        "  print(f'Path: {images_path}\\nThe above path does not exist!')"
      ],
      "metadata": {
        "id": "ng4rS5lHruAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Option 1: Download a specific image from a url (there are several options, we'll show one): requests** "
      ],
      "metadata": {
        "id": "_dYMkN-5sCuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to download an image from a url\n",
        "\n",
        "import requests # lib to request image from web\n",
        "import shutil # lib to save the image locally\n",
        "\n",
        "\n",
        "def downloadImageFromURL (url='', destination_image_path=''):\n",
        "\n",
        "  # The method will take in two parameters, the url variable you created earlier, and stream: True.\n",
        "  # by adding this second argument in guarantees no interruptions will occur when the method is running.\n",
        "  res = requests.get(url, stream = True) \n",
        "\n",
        "  if res.status_code == 200:\n",
        "      with open(destination_image_path,'wb') as f:\n",
        "          shutil.copyfileobj(res.raw, f)\n",
        "      print('Image sucessfully Downloaded: ', destination_image_path)\n",
        "  else:\n",
        "      print('Image Couldn\\'t be retrieved')\n",
        "# lets download an image of a plant\n",
        "url_watermelon = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcToGHpqiMEfRT8UZlTHbbxL9xpEqqXpTucC7w&usqp=CAU'\n",
        "path_to_save_image = f'{images_path}/watermelon_plant.jpg'\n",
        "downloadImageFromURL(url_watermelon, path_to_save_image)"
      ],
      "metadata": {
        "id": "rllJDu_pr_h0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Option 2: Download an image directly from Google Image Search:**"
      ],
      "metadata": {
        "id": "xMVGhTc3sc5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first uninstall the lib that colab has (enter 'y' to uninstall, and restart runtime)\n",
        "!pip uninstall google_images_download\n",
        "\n",
        "# then install the updated library\n",
        "!pip install git+https://github.com/Joeclinton1/google-images-download.git\n",
        "# imports libs (again, since runtime was restarted)\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "import skimage.io as io\n",
        "\n",
        "# import the newly installed library\n",
        "from google_images_download import google_images_download\n",
        "\n",
        "# function to download images from Google Image Search\n",
        "def downloadImagesFromGoogle(words_to_search='', number_of_imgs=3):\n",
        "\n",
        "  response = google_images_download.googleimagesdownload() #instantiate the class\n",
        "  arguments = {\"keywords\":words_to_search,\n",
        "              \"limit\":number_of_imgs,\"print_urls\":False}\n",
        "  paths = response.download(arguments)  \n",
        "  print(paths) #print complete paths to the downloaded images\n",
        "  # calling our function above (images will be downloaded to the 'downloads' folder on the left)\n",
        "downloadImagesFromGoogle('corn plant,watermelon plant', 3)\n",
        "# lets read the images we downloaded into a list\n",
        "import glob\n",
        "watermelon_paths = glob.glob('/content/downloads/watermelon plant/*.*') # get the paths of the watermelon images\n",
        "# read images into a list\n",
        "img_list = []\n",
        "for path in watermelon_paths:\n",
        "  img = io.imread(path)\n",
        "  img_list.append(img)\n",
        "# displaying the downloaded images\n",
        "fig, ax=plt.subplots(ncols=len(img_list), nrows=1, figsize=(8, 8))\n",
        "plt.tight_layout() # nicer layout\n",
        "# loop to plt.imshow all the images in one row\n",
        "for i in range(len(img_list)):\n",
        "  ax[i].imshow(img_list[i])\n",
        "  ax[i].set_title(f'Image {i+1}')"
      ],
      "metadata": {
        "id": "Mh9eCOzosb92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One channel histogram:\n",
        "   hist = cv.calcHist(images, channels, mask, histSize, ranges[, hist[, accumulate]])\n",
        "1.   **images** : it is the source image of type uint8 or float32. it should be given in square brackets, ie, \"[img]\".\n",
        "2.   **channels** : it is also given in square brackets. It is the index of channel for which we calculate histogram. For example, if input is grayscale image, its value is [0]. For color image, you can pass [0], [1] or [2] to calculate histogram of blue, green or red channel respectively.\n",
        "3. **mask** : mask image. To find histogram of full image, it is given as \"None\". But if you want to find histogram of particular region of image, you have to create a mask image for that and give it as mask.\n",
        "4. **histSize** : this represents our BIN count. Need to be given in square brackets. For full scale, we pass [256].\n",
        "5. **ranges** : this is our RANGE. Normally, it is [0,256]. (the hist and accumalte inside the ranges, enables to compute a single histogram from several sets of arrays.)\n"
      ],
      "metadata": {
        "id": "JGE-Rm8YtWNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# computing the histogram of the red channel of our watermelon image\n",
        "img = img_list[0].copy()\n",
        "hist = cv2.calcHist([img],[0],None,[256],[0,256]) # our image, first channel(0=red), no mask, 256 bins, range 0-255\n",
        "  \n",
        "# plot the above computed histogram\n",
        "plt.plot(hist, color='r') # red color for the line\n",
        "plt.title('Image histogram for the red channel of our watermelon image', y=1.03) # the y makes the title a bit higher\n",
        "plt.xlabel(\"Pixel Value\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j8d8gn6CtVU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple channels:"
      ],
      "metadata": {
        "id": "wdrfkAjmuQ9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# computing the histogram of all channels of our watermelon image\n",
        "img = img_list[0].copy()\n",
        "  \n",
        "# plot the above computed histogram\n",
        "colors = ('r','g','b') # for the line color\n",
        "\n",
        "for i,color in enumerate(colors):\n",
        "  hist = cv2.calcHist([img],[i],None,[256],[0,256]) # our image, channel(0/1/2), no mask, 256 bins, range 0-255\n",
        "  plt.plot(hist, color = color) # r/g/b color for each line\n",
        "plt.title('Image histogram (RGB) of our watermelon image', y=1.03) # the y makes the title a bit higher\n",
        "plt.xlabel(\"Pixel Value\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "enSSHsBuuN1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using plt to plot a histogram (one channel)\n",
        "\n",
        "hist_with_plt = plt.hist(img[:,:,1].flatten(), bins = 256, color='g')\n",
        "plt.title('Image histogram (green layer) of our watermelon image', y=1.03) # the y makes the title a bit higher\n",
        "plt.xlabel(\"Pixel Value\")\n",
        "plt.ylabel(\"Pixel Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Buwv_klsub4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using plt to plot a histogram (multi channel)\n",
        "\n",
        "colors = ('r','g','b') # for the line color\n",
        "\n",
        "hist = plt.hist(img.flatten(), bins = 256, color = 'orange') # plotting the total histogram\n",
        "\n",
        "#loop to plot for each channel\n",
        "for i,color in enumerate(colors):\n",
        "  hist = plt.hist(img[:, :, i].flatten(), bins = 256, color = color, alpha = 0.5)\n",
        "\n",
        "# some more details for the figure\n",
        "plt.title('Image histogram (RGB) of our watermelon image', y=1.03) # the y makes the title a bit higher\n",
        "plt.xlabel(\"Pixel Value\")\n",
        "plt.ylabel(\"Pixel Count\")\n",
        "plt.legend(['Total', 'Red Channel', 'Green Channel', 'Blue Channel']) # adding a legend\n",
        "plt.show()\n",
        "# adding the images together\n",
        "# Try to imagine the outcome before you do the actual calculation. What do you think is going to happen?\n",
        "\n",
        "if images_list[1].shape == resized2.shape:\n",
        "  added_img = images_list[1] + resized2\n",
        "  plt.imshow(added_img)\n",
        "  plt.title('Two images added together')\n",
        "else:\n",
        "  print('Shapes are different, cannot add.')"
      ],
      "metadata": {
        "id": "30s_oE4susyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#class 4\n",
        "https://colab.research.google.com/drive/1p8B8qEMT-23KURdiPVjmLtHrukyxuGBO?authuser=1"
      ],
      "metadata": {
        "id": "OaLOUSYxvd2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##mean filter"
      ],
      "metadata": {
        "id": "X_3K8yNCxkky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_kernel = np.full((3, 3), 1/9) # defining our mean kernel (3,3) with values of 1/9\n",
        "import scipy.ndimage as ndi\n",
        "\n",
        "%precision 2\n",
        "print('Our original array:\\n',bright_square)\n",
        "print('\\nOur convolved array:\\n',ndi.correlate(bright_square, mean_kernel))"
      ],
      "metadata": {
        "id": "rNm8dspnvg8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##usefull"
      ],
      "metadata": {
        "id": "8gZkQ8U1yMro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage import img_as_float\n",
        "\n",
        "def imshow_all(images, titles=None):\n",
        "    images = [img_as_float(img) for img in images]\n",
        "\n",
        "    if titles is None:\n",
        "        titles = [''] * len(images)\n",
        "    vmin = min(map(np.min, images))\n",
        "    vmax = max(map(np.max, images))\n",
        "    ncols = len(images)\n",
        "    height = 5\n",
        "    width = height * len(images)\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=ncols,\n",
        "                             figsize=(width, height))\n",
        "    for ax, img, label in zip(axes.ravel(), images, titles):\n",
        "        ax.imshow(img, vmin=vmin, vmax=vmax, cmap = 'gray')\n",
        "        ax.set_title(label)\n"
      ],
      "metadata": {
        "id": "SvTjkNQtyJld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##gaussian"
      ],
      "metadata": {
        "id": "xfDvmsLjy0Ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage import img_as_float\n",
        "# The Gaussian filter returns a float image, regardless of input.\n",
        "# Cast to float so the images have comparable intensity ranges.\n",
        "\n",
        "pixelated_float = img_as_float(pixelated)\n",
        "smooth = filters.gaussian(pixelated_float, sigma=1)\n",
        "imshow_all([pixelated_float, smooth],titles=['pixelated', 'result of gaussian filter'])"
      ],
      "metadata": {
        "id": "ID3chfnayUVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets apply the gaussian filter\n",
        "img = cropped_img_gaussian_noise.copy()\n",
        "\n",
        "# using OpenCV\n",
        "# agruments: image, kernel size, sigma values, border type (padding, etc.)\n",
        "# cv2.BORDER_CONSTANT adds a constant color border\n",
        "gaussian_using_cv2 = cv2.GaussianBlur(img, (3,3), 0, borderType=cv2.BORDER_CONSTANT) \n",
        "\n",
        "# using skimage\n",
        "# sigma defines the std dev of the gaussian kernel\n",
        "# cval is a value to fill past edges of input if mode is ‘constant’. Default is 0.0\n",
        "gaussian_using_skimage = skimage.filters.gaussian(img, sigma=1, mode='constant', cval=0.0)"
      ],
      "metadata": {
        "id": "4hdCb5cky5PC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unsharp Mask"
      ],
      "metadata": {
        "id": "eXs2Em59zQuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# applying the Unsharp Mask\n",
        "gaussian_img = gaussian(img, sigma=2, mode='constant', cval=0.0) # creating our blured version\n",
        "\n",
        "# enhanced image = original + amount * (original - blurred)\n",
        "enhanced_image = img + (img - gaussian_img)*1.\n",
        "\n",
        "# displaying the filtered images\n",
        "imshow_all([img[100:200,100:200], enhanced_image[100:200,100:200]],\n",
        "           ['Original', 'Unsharp Mask'])"
      ],
      "metadata": {
        "id": "qDAu2VtrzFqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic edge filtering"
      ],
      "metadata": {
        "id": "Gj-HRpMbzhnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lets create a vertical kernel\n",
        "vertical_kernel = np.array([\n",
        "    [-1],\n",
        "    [ 0],\n",
        "    [ 1],\n",
        "])\n",
        "\n",
        "# convolve our above image\n",
        "gradient_vertical = ndi.correlate(pixelated.astype(float),\n",
        "                                  vertical_kernel)\n",
        "# display\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(gradient_vertical, cmap='gray')\n",
        "ax[1].imshow(pixelated, cmap='gray')"
      ],
      "metadata": {
        "id": "W-j7Y-4KzbBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sobel edge filter"
      ],
      "metadata": {
        "id": "xeb9WwBEzoxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# see what happens when we apply sobel to a noisy image\n",
        "pixelated_gradient = filters.sobel(pixelated)\n",
        "imshow_all([pixelated, pixelated_gradient])\n",
        "# applying the sobel after smoothing\n",
        "\n",
        "gradient = filters.sobel(smooth) # applying sobel to our above smoothed version\n",
        "titles = ['gradient before smoothing', 'gradient after smoothing']\n",
        "\n",
        "# Scale smoothed gradient up so they're of comparable brightness.\n",
        "imshow_all([pixelated_gradient, gradient*1.8], titles=titles)\n"
      ],
      "metadata": {
        "id": "zLciLl-qzoX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Canny Edge Detector"
      ],
      "metadata": {
        "id": "iMP-GJMJzoOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying Canny\n",
        "canny_edge = cv2.Canny(img, 195, 200)  #Supply Thresholds 1 and 2 \n",
        "\n",
        "plt.imshow(canny_edge, cmap = 'gray')\n"
      ],
      "metadata": {
        "id": "G67kd3rAz4To"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#class 5\n",
        "https://colab.research.google.com/drive/14F8H5_kS3J6Vl2fk_kiRIMp92KIqLfXJ?authuser=1"
      ],
      "metadata": {
        "id": "ici9pIQS0Qi7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##segmantion using otsu"
      ],
      "metadata": {
        "id": "NoWcVUrs0mJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using cv2 for otsu based automatic thresholding\n",
        "\n",
        "from skimage import img_as_ubyte\n",
        "ret2, thresh2 = cv2.threshold(img_as_ubyte(img),0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
        "print('Threshold value by otsu is: ', ret2)\n",
        "\n",
        "plt.imshow(thresh2, cmap='gray')\n",
        "plt.title('Our mask, using cv2+otsu')"
      ],
      "metadata": {
        "id": "anEtiWeL0Vpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using skimage for otsu based automatic thresholding\n",
        "# The skimage.filters.threshold_otsu() function can be used to determine the threshold automatically via Otsu’s method\n",
        "import skimage\n",
        "import skimage.filters\n",
        "\n",
        "# perform automatic thresholding\n",
        "t = skimage.filters.threshold_otsu(img)\n",
        "print(\"Found automatic threshold t=\", t, f' ({255*t})\\n')\n",
        "\n",
        "# create a binary mask with the threshold found by Otsu's method\n",
        "binary_mask = img > t\n",
        "\n",
        "# display\n",
        "plt.imshow(binary_mask, cmap='gray')\n",
        "plt.title('Our mask, using skimage+otsu')"
      ],
      "metadata": {
        "id": "eP6-wGMr0qsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Video Playlist: https://www.youtube.com/playlist?list=PLHae9ggVvqPgyRQQOtENr6hK0m1UquGaG\n",
        "\n",
        "# Image segmentation and morphological operators\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from skimage.filters import threshold_multiotsu\n",
        "import cv2\n",
        "\n",
        "# Apply multi-Otsu threshold \n",
        "thresholds = threshold_multiotsu(img, classes=4)\n",
        "print(thresholds)\n",
        "# Digitize (segment) original image into multiple classes.\n",
        "# np.digitize assign values 0, 1, 2, 3, ... to pixels in each class.\n",
        "regions = np.digitize(img, bins=thresholds)\n",
        "plt.imshow(regions)\n",
        "# Lets take a look at region 3\n",
        "plt.imshow(regions==3, cmap='gray')\n"
      ],
      "metadata": {
        "id": "hTMwclPX0wet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using texture to segment images"
      ],
      "metadata": {
        "id": "EdOb0LyX1DO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Entropy -  quantifies disorder.\n",
        "\n",
        "# lets take a look at the left colony\n",
        "from skimage.filters.rank import entropy\n",
        "from skimage.morphology import disk\n",
        "left_img = img.copy()[:,0:900] # crop the left cell\n",
        "entropy_img = entropy(left_img, disk(9)) # play with the disk size to get better results\n",
        "plt.imshow(entropy_img)\n",
        "\n",
        "#Scratch Analysis - single image\n",
        "\n",
        "#Now let us use otsu to threshold high vs low entropy regions.\n",
        "plt.hist(entropy_img.flat, bins=100, range=(0,5))  #.flat returns the flattened numpy array (1D)\n",
        "plt.show()\n",
        "thresh = threshold_otsu(entropy_img)\n",
        "print(thresh)\n",
        "\n",
        "#Now let us binarize the entropy image \n",
        "binary = entropy_img <= thresh\n",
        "\n",
        "#display\n",
        "plt.imshow(binary)\n",
        "\n",
        "# a method to fill in holes\n",
        "\n",
        "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(12,12)) # define a kernel (change size if needed)\n",
        "res = cv2.morphologyEx(img_as_ubyte(binary),cv2.MORPH_OPEN,kernel) # applying the kernel to our binary (make sure it's not a boolean array)\n",
        "\n",
        "# display the results\n",
        "plt.imshow(res, cmap='gray')\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "rwxn-llG1F4j",
        "outputId": "bd79a353-d2b4-4eca-bef3-9ba7a41e0946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-9f1c9724a9ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmorphology\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mleft_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m900\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# crop the left cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mentropy_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# play with the disk size to get better results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentropy_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'img' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Image segmentation using color spaces**"
      ],
      "metadata": {
        "id": "-5vNNAdw2JFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to hsv color space\n",
        "hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV) # convert rgb to hsv\n",
        "\n",
        "mask = cv2.inRange(hsv, (100,90,90), (120,255,255)) # mask out the blue balls. range of hsv. from hsv1 to hsv2. \n",
        "# enter the link to see the hsv color space\n",
        "# https://stackoverflow.com/questions/10948589/choosing-the-correct-upper-and-lower-hsv-boundaries-for-color-detection-withcv/48367205#48367205\n",
        "# we chose only h and s. v we made same as s\n",
        "\n",
        "#mask = cv2.inRange(hsv, (0,0,180), (180,70,255)) # White\n",
        "\n",
        "# display our mask\n",
        "plt.imshow(mask,cmap='gray')\n",
        "# we have holes\n",
        "# lets close them with the binary closing\n",
        "from scipy import ndimage as nd\n",
        "closed_mask = nd.binary_closing(mask, np.ones((5,5)))\n",
        "\n",
        "# display our mask\n",
        "plt.imshow(closed_mask,cmap='Blues')\n",
        "# lets label our blue balls, each one will get a different color\n",
        "label_image = measure.label(closed_mask)\n",
        "plt.imshow(label_image)"
      ],
      "metadata": {
        "id": "Uq-_Eldy2JVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#class 6"
      ],
      "metadata": {
        "id": "ekmSWXLYj4Qo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Image segmentation using watershed"
      ],
      "metadata": {
        "id": "6risS1ufBdxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# images path dec\n",
        "images_path = r'/content/drive/MyDrive/71254_2023/01_Lectures/Class06/images'\n",
        "\n",
        "# import libs\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "from scipy import ndimage\n",
        "from skimage.feature import peak_local_max\n",
        "from skimage.segmentation import watershed\n",
        "from skimage import io, img_as_ubyte\n",
        "\n",
        "'''\n",
        "The imutils library is a series of convenience functions to make basic image processing functions such as translation,\n",
        "rotation, resizing, skeletonization, and displaying Matplotlib images easier with OpenCV and Python 3.'''\n",
        "import imutils\n",
        "\n",
        "\n",
        "# pre-processing\n",
        "img = cv2.imread(f'{images_path}/watershed_coins.jpg') # read image\n",
        "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # bgr to rgb\n",
        "plt.imshow(img_rgb)"
      ],
      "metadata": {
        "id": "sr_2-08IBhuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# continued\n",
        "\n",
        "shifted = cv2.pyrMeanShiftFiltering(img, 21, 51) # mean shift filtering to aid the thresholding step. only for rgb. read more here: https://docs.opencv.org/2.4/modules/imgproc/doc/filtering.html#pyrmeanshiftfiltering\n",
        "gray = cv2.cvtColor(shifted, cv2.COLOR_BGR2GRAY) # color to gray\n",
        "thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
        "\n",
        "# Now let us visualize the output result\n",
        "figure_size = 15\n",
        "plt.figure(figsize=(figure_size,figure_size))\n",
        "\n",
        "plt.subplot(1,3,1),plt.imshow(img_rgb)\n",
        "plt.title('Original Image')\n",
        "\n",
        "plt.subplot(1,3,2),plt.imshow(shifted, cmap='gray')\n",
        "plt.title('Mean Shifted')\n",
        "\n",
        "plt.subplot(1,3,3),plt.imshow(thresh, cmap='gray')\n",
        "plt.title('Thresholded')"
      ],
      "metadata": {
        "id": "Em2rPMNuBpxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the exact Euclidean distance from every binary\n",
        "# pixel to the nearest zero pixel, then find peaks in this\n",
        "# distance map\n",
        "D = ndimage.distance_transform_edt(thresh)\n",
        "\n",
        "# Now we take D , our distance map, and find peaks (i.e., local maxima) in the map. We’ll ensure that is at least a 20 pixel distance between each peak.\n",
        "localMax = peak_local_max(D, indices=False, min_distance=10,\n",
        "\tlabels=thresh)"
      ],
      "metadata": {
        "id": "j1VPHiAhBmPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perform a connected component analysis on the local peaks,\n",
        "# using 8-connectivity, then appy the Watershed algorithm\n",
        "markers = ndimage.label(localMax, structure=np.ones((3, 3)))[0]\n",
        "labels = watershed(-D, markers, mask=thresh) #  matrix of labels\n",
        "print(\"[INFO] {} unique segments found\".format(len(np.unique(labels)) - 1))"
      ],
      "metadata": {
        "id": "hrZ8e8PTBu_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loop over the unique labels returned by the Watershed algorithm\n",
        "total_area = [] # list to store areas of onjects\n",
        "\n",
        "for i,label in enumerate(np.unique(labels)):\n",
        "\t# if the label is zero, we are examining the 'background'\n",
        "\t# so simply ignore it\n",
        "\tif label == 0:\n",
        "\t\tcontinue\n",
        "\t# otherwise, allocate memory for the label region, and set the pixels belonging to the current label to 255 (white). draw it on the mask.\n",
        "\tmask = np.zeros(gray.shape, dtype=\"uint8\")\n",
        "\tmask[labels == label] = 255\n",
        "\n",
        "\n",
        "\t# detect contours in the mask and grab the largest one — this contour will represent the outline/boundary of a given object in the image.\n",
        "\tcnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL,\n",
        "\t\tcv2.CHAIN_APPROX_SIMPLE)\n",
        "\tcnts = imutils.grab_contours(cnts)\n",
        "\tc = max(cnts, key=cv2.contourArea)\n",
        "\t\n",
        "\t# calc area and append to list\n",
        "\tarea = cv2.contourArea(c)\n",
        "\ttotal_area.append(area)\n",
        "\tprint(f'Object number {i} has an area = ', area)\n",
        " \t\n",
        " \n",
        "\t# draw the contours enclosing the object\n",
        "\t((x, y), r) = cv2.minEnclosingCircle(c)  # We find the circumcircle of an object using the function cv.minEnclosingCircle(). It is a circle which completely covers the object with minimum area.\n",
        "\tcv2.putText(img_rgb, \"#{}\".format(label), (int(x) - 15 , int(y)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2) # draw the ID on the object\n",
        "\tcv2.drawContours(img_rgb, cnts, -1, (0,255,0), 1) # draw the counters\n",
        " \n",
        "# show the output image\n",
        "plt.imshow(img_rgb)"
      ],
      "metadata": {
        "id": "gN5fMHDkB2aG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Image segmentation using Voronoi**"
      ],
      "metadata": {
        "id": "Bl_oBGSSCCaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# images path dec\n",
        "images_path = r'/content/drive/MyDrive/71254_2023/01_Lectures/Class06/images'\n",
        "\n",
        "# import libs\n",
        "from skimage import io, filters\n",
        "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# read an image of cells\n",
        "img = io.imread(f'{images_path}/cells.jpg', as_gray = True)\n",
        "\n",
        "# display it\n",
        "plt.imshow(img, cmap='gray')"
      ],
      "metadata": {
        "id": "TS56l02VCGyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1 - Gaussian blur to average local intensity variations (it will help out locate the cells in the image)\n",
        "img_blurred = filters.gaussian(img, sigma=5)\n",
        "plt.imshow(img_blurred, cmap='gray')"
      ],
      "metadata": {
        "id": "KQhEk7zVCHjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Find the points representing each object, to be used for Voronoi\n",
        "from skimage.feature import peak_local_max\n",
        "coordinates = peak_local_max(img, min_distance=20, \n",
        "                             exclude_border=False)\n",
        "\n",
        "# display the local max points on the blurred image\n",
        "plt.imshow(img_blurred, cmap='gray')\n",
        "\n",
        "# Plot y versus x as markers\n",
        "# 'r.' draws the x,y as red points\n",
        "plt.plot(coordinates[:, 1], coordinates[:, 0], 'r.') "
      ],
      "metadata": {
        "id": "3VJArBWXCM3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Voronoi regions - we'll use the above points as seeds\n",
        "vor3 = Voronoi(coordinates) # our Voronoi diagram based on the seeds\n",
        "fig3 = voronoi_plot_2d(vor3) # creating a figure from the Voronoi diagram, so we can plot it\n",
        "plt.show() # it will show fig3"
      ],
      "metadata": {
        "id": "ApNr4bvFCPfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Using pyclesperanto package to segment based on Voronoi+Otsu - SHORT**"
      ],
      "metadata": {
        "id": "b2FfjXauC8wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# otsu + vor + labeling\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pyclesperanto_prototype as cle\n",
        "from skimage import exposure, img_as_ubyte\n",
        "\n",
        "#Read the input image\n",
        "# read an image of cells\n",
        "img = io.imread(f'{images_path}/cells.jpg', as_gray = True)\n",
        "\n",
        "# display it\n",
        "plt.imshow(img, cmap='gray')\n",
        "\n",
        "#Normalize then scale to 255 and convert to uint8 - using skimage\n",
        "cells_8bit = img_as_ubyte(img)\n",
        "plt.imshow(cells_8bit, cmap='gray')"
      ],
      "metadata": {
        "id": "uvTmgKDeC-ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preparing our GPU\n",
        "\n",
        "# list names of all available GPU-devices\n",
        "print(\"Available devices:\" + str(cle.available_device_names()))"
      ],
      "metadata": {
        "id": "FwOx0G1vDAtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select a specific GPU device from the above\n",
        "device = cle.select_device('Tesla T4')\n",
        "print(\"Used GPU: \", device)\n",
        "\n",
        "#Push the image to gpu memory\n",
        "cells_gpu = cle.push(cells_8bit)\n",
        "print(\"Image size in GPU: \" + str(cells_gpu.shape))\n",
        "\n",
        "# display\n",
        "cle.imshow(cells_8bit, color_map='gray')"
      ],
      "metadata": {
        "id": "VNMXXoO-DDkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############ voronoi_otsu_labeling library ##################\n",
        "# voronoi_otsu_labeling(image, spot_sigma=some_number, outline_sigma=another_number)\n",
        "#spot_sigma= depends on how close the detected objects can be. Low number may divide large objects into multiple objects.\n",
        "#outline_sigma = how precise the outline needs to be for the segmented objects (use a low number)\n",
        "segmented = cle.voronoi_otsu_labeling(cells_gpu, spot_sigma=5, \n",
        "                                      outline_sigma=1)\n",
        "cle.imshow(segmented, labels=True)"
      ],
      "metadata": {
        "id": "Nr9UNSM9DHFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Image segmentation using K-Means**"
      ],
      "metadata": {
        "id": "-lmM0nkvDYrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#k means\n",
        "\n",
        "__author__ = \"Sreenivas Bhattiprolu\"\n",
        "__license__ = \"Feel free to copy, I appreciate if you acknowledge Python for Microscopists\"\n",
        "# https://www.youtube.com/watch?v=6CqRnx6Ic48\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "\n",
        "# read an image of pebbles\n",
        "img = io.imread(f'{images_path}/sand.jpg', as_gray = False)\n",
        "\n",
        "# display it\n",
        "plt.imshow(img, cmap='gray')"
      ],
      "metadata": {
        "id": "lNG1ZQgADdmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert MxNx3 image into Kx3 where K=MxN\n",
        "pixel_values  = img.reshape((-1,3))  #-1 reshape means, in this case MxN\n",
        "\n",
        "#We convert the unit8 values to float as it is a requirement of the k-means method of OpenCV\n",
        "pixel_values = np.float32(pixel_values)"
      ],
      "metadata": {
        "id": "H7u662YYDgg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pixel_values.shape)\n",
        "print(f'As expected, this results from flattening a high resolution {img.shape} image.')"
      ],
      "metadata": {
        "id": "Uqb6tfksDiq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define stopping criteria\n",
        "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)"
      ],
      "metadata": {
        "id": "rOOYAdgjDkdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of clusters (K)\n",
        "k = 4"
      ],
      "metadata": {
        "id": "gjpXURInDoga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attempts = 10\n",
        "_, labels, (centers) = cv2.kmeans(pixel_values, k, None, criteria, attempts, cv2.KMEANS_RANDOM_CENTERS)"
      ],
      "metadata": {
        "id": "LWxaVr5rDpUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert back to 8 bit values\n",
        "centers = np.uint8(centers)\n",
        "\n",
        "# flatten the labels array\n",
        "labels = labels.flatten()"
      ],
      "metadata": {
        "id": "LtjP6sFLDrgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert all pixels to the color of the centroids\n",
        "segmented_image = centers[labels.flatten()]"
      ],
      "metadata": {
        "id": "3B9sQls4Dtbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reshape back to the original image dimension\n",
        "segmented_image = segmented_image.reshape(img.shape)\n",
        "# show the image\n",
        "plt.imshow(segmented_image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2A55QVFSDvub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we take a look at the centers of the diff objects\n",
        "print(centers)\n",
        "\n",
        "# from trial and error, I found that the row ([90,103,55]) is the grass!"
      ],
      "metadata": {
        "id": "37DrucmCD6SD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets get the index of the grass row \n",
        "\n",
        "for i,center in enumerate(centers):\n",
        "  if np.all(center == ([90,103,55])):\n",
        "    grass_center_index = i\n",
        "    print(grass_center_index)"
      ],
      "metadata": {
        "id": "hUmiQOsND8jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# copy source img\n",
        "masked_image = img.copy()\n",
        "\n",
        "# convert to the shape of a vector of pixel values (like suits for kmeans)\n",
        "masked_image = masked_image.reshape((-1, 3))\n",
        "\n",
        "# color (i.e cluster) to exclude\n",
        "list_of_cluster_numbers_to_exclude = list(range(k)) # create a list that has the number from 0 to k-1\n",
        "list_of_cluster_numbers_to_exclude.remove(grass_center_index) # remove the cluster of grass that we want to keep, and not black out\n",
        "for cluster in list_of_cluster_numbers_to_exclude:\n",
        "  masked_image[labels== cluster] = [0, 0, 0] # black all clusters except cluster grass_center_index\n",
        "\n",
        "# convert back to original shape\n",
        "masked_image = masked_image.reshape(img.shape)\n",
        "\n",
        "# show the image\n",
        "plt.imshow(masked_image)"
      ],
      "metadata": {
        "id": "JUGXeqiiD_by"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# detect contours in an image and grab the largest one — this contour will represent the outline/boundary of a given object in the image.\n",
        "gray = cv2.cvtColor(masked_image, cv2.COLOR_RGB2GRAY) # color to gray\n",
        "thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY)[1] # threshold the gray\n",
        "cnts = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) # find the boundaries \n",
        "cnts = imutils.grab_contours(cnts) \n",
        "c = max(cnts, key=cv2.contourArea)\n",
        "\n",
        "# draw the contours enclosing the object\n",
        "cv2.drawContours(img, cnts, -1, (0,255,0), 1) # draw the counters\n",
        "\n",
        "# calc number of pixels within the object and append to list\n",
        "area = cv2.contourArea(c)\n",
        "print(f'Object has an area = ', area, 'pixels')\n",
        "\n",
        "plt.imshow(img, cmap='Greens')"
      ],
      "metadata": {
        "id": "6BKRMkx9EBjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the image with area\n",
        "plt.imshow(masked_image)\n",
        "plt.title(f'Segmented object area: {area} pixels')"
      ],
      "metadata": {
        "id": "S-hTkhnFEGhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Colab Forms"
      ],
      "metadata": {
        "id": "QMZklc0rEP2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title String fields\n",
        "\n",
        "text = 'serrerydrtyrytsetse' #@param {type:\"string\"}\n",
        "dropdown = '2nd option' #@param [\"1st option\", \"2nd option\", \"3rd option\"]\n",
        "text_and_dropdown = 'setste' #@param [\"1st option\", \"2nd option\", \"3rd option\"] {allow-input: true}\n",
        "\n",
        "print(text)\n",
        "print(dropdown)\n",
        "print(text_and_dropdown)"
      ],
      "metadata": {
        "id": "JVWQ9xdOEQ13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Date fields\n",
        "date_input = '2018-03-04' #@param {type:\"date\"}\n",
        "\n",
        "print(date_input)"
      ],
      "metadata": {
        "id": "tgmOWcdUESzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Number fields\n",
        "number_input = 10.0 #@param {type:\"number\"}\n",
        "number_slider = -1 #@param {type:\"slider\", min:-1, max:1, step:0.1}\n",
        "\n",
        "integer_input = 10 #@param {type:\"integer\"}\n",
        "integer_slider = 47 #@param {type:\"slider\", min:0, max:100, step:1}\n",
        "\n",
        "print(number_input)\n",
        "print(number_slider)\n",
        "\n",
        "print(integer_input)\n",
        "print(integer_slider)"
      ],
      "metadata": {
        "id": "AQxGTLPVEUvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Uploading files or images**\n",
        "\n",
        "You can ask the user to upload files through a browse button, using google.colab.files.upload()"
      ],
      "metadata": {
        "id": "g5KCryyxEeQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title UPLOAD IMAGES HERE: RUN ME.  { display-mode: \"form\" }\n",
        "from google.colab import files\n",
        "\n",
        "try:\n",
        "  uploaded = files.upload()\n",
        "except:\n",
        "  print(\"\")\n",
        "  print(\"Please use Chrome, and enable cookies!\")\n",
        "  print(\"cookie אנא היכנסו דרך דפדפן כרום במחשב, והפעילו גישה לקבצי \")\n",
        "\n",
        "# lets the user upload the file, and stores the file name in 'file_names'. The files are uploaded under /content/FILE_NAME\n",
        "file_names = uploaded.keys() "
      ],
      "metadata": {
        "id": "PJyRJEhUEfyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets convert our file names into a list of of file names\n",
        "list_file_paths = list(file_names)\n",
        "list_file_paths"
      ],
      "metadata": {
        "id": "iP0QEWuwEnKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Downloading files: Example, downloading the first uploaded file above. RUN ME!  { display-mode: \"form\" }\n",
        "files.download(list_file_paths[0])"
      ],
      "metadata": {
        "id": "AY_6FESCEsxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#class 7"
      ],
      "metadata": {
        "id": "cqN_-XSjj7wp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Measure properties with AruCo Markers"
      ],
      "metadata": {
        "id": "tqKrR8abFLTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-contrib-python"
      ],
      "metadata": {
        "id": "h-s6ywYqFMO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import libs\n",
        "from skimage import measure, io, img_as_ubyte, morphology, util, color\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.color import label2rgb, rgb2gray\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import imutils"
      ],
      "metadata": {
        "id": "sZQauWwdFPFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# folder path - input from user (since sometimes you students forget to change this :P)\n",
        "# mine (Yedidya) is: /content/drive/MyDrive/71254_2023/01_Lectures/Class07\n",
        "folder_path = input('What is your folder path for this project?\\n') "
      ],
      "metadata": {
        "id": "FhsxEZBIFQ1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read an image\n",
        "image =io.imread(f'{folder_path}/images/tomato.jpg')\n",
        "grayscale = img_as_ubyte(rgb2gray(io.imread(f'{folder_path}/images/tomato.jpg')))\n",
        "plt.imshow(image)"
      ],
      "metadata": {
        "id": "hFE_Hz_0FVaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Aruco detector\n",
        "parameters = cv2.aruco.DetectorParameters_create()\n",
        "aruco_dict = cv2.aruco.Dictionary_get(cv2.aruco.DICT_5X5_50)"
      ],
      "metadata": {
        "id": "FWYYCCGhFYJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Aruco marker\n",
        "corners, _, _ = cv2.aruco.detectMarkers(image, aruco_dict, parameters=parameters)"
      ],
      "metadata": {
        "id": "ozDW14BQFbEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Draw polygon around the marker\n",
        "int_corners = np.int0(corners)\n",
        "cv2.polylines(image, int_corners, True, (0, 255, 0), 50)\n",
        "plt.imshow(image)"
      ],
      "metadata": {
        "id": "8PR2ez3pFduI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aruco Area\n",
        "aruco_area = cv2.contourArea (corners[0])\n",
        "print('AruCo Area:',aruco_area, 'px')\n",
        "\n",
        "# Pixel to cm ratio\n",
        "pixel_cm_ratio = 5*5 / aruco_area# since the AruCo is 5*5 cm, so we devide 25 cm*cm by the number of pixels\n",
        "print('Ratio - Each pixel is',pixel_cm_ratio, 'cm*cm')"
      ],
      "metadata": {
        "id": "QUk4qnmfFfrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Segmenting with K-Means (we learned in Class 06) + Calculate Area"
      ],
      "metadata": {
        "id": "Hjg8lSWCFmXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to segment using k-means\n",
        "\n",
        "def segment_image_kmeans(img, k=3, attempts=10): \n",
        "\n",
        "    # Convert MxNx3 image into Kx3 where K=MxN\n",
        "    pixel_values  = img.reshape((-1,3))  #-1 reshape means, in this case MxN\n",
        "\n",
        "    #We convert the unit8 values to float as it is a requirement of the k-means method of OpenCV\n",
        "    pixel_values = np.float32(pixel_values)\n",
        "\n",
        "    # define stopping criteria\n",
        "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)\n",
        "    \n",
        "    _, labels, (centers) = cv2.kmeans(pixel_values, k, None, criteria, attempts, cv2.KMEANS_RANDOM_CENTERS)\n",
        "    \n",
        "    # convert back to 8 bit values\n",
        "    centers = np.uint8(centers)\n",
        "\n",
        "    # flatten the labels array\n",
        "    labels = labels.flatten()\n",
        "    \n",
        "    # convert all pixels to the color of the centroids\n",
        "    segmented_image = centers[labels.flatten()]\n",
        "    \n",
        "    # reshape back to the original image dimension\n",
        "    segmented_image = segmented_image.reshape(img.shape)\n",
        "    \n",
        "    return segmented_image, labels, centers"
      ],
      "metadata": {
        "id": "_m1WBEsbFi37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# segment using kmeans\n",
        "image =io.imread(f'{folder_path}/images/tomato.jpg')\n",
        "k=3\n",
        "attempts=10\n",
        "segmented_kmeans, labels, centers = segment_image_kmeans(image, k, attempts)\n",
        "plt.imshow(segmented_kmeans)"
      ],
      "metadata": {
        "id": "_tHy-JWkFpV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(centers)"
      ],
      "metadata": {
        "id": "IkXtrI_yFslR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets get the index of the leafs row \n",
        "\n",
        "for i,center in enumerate(centers):\n",
        "  if np.all(center == ([133,165,26])):\n",
        "    leaf_center_index = i\n",
        "    print(leaf_center_index)"
      ],
      "metadata": {
        "id": "ptEkTLk5F0e5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# copy source img\n",
        "img = image.copy()\n",
        "masked_image = img.copy()\n",
        "\n",
        "# convert to the shape of a vector of pixel values (like suits for kmeans)\n",
        "masked_image = masked_image.reshape((-1, 3))\n",
        "\n",
        "index_to_remove = leaf_center_index\n",
        "\n",
        "# color (i.e cluster) to exclude\n",
        "list_of_cluster_numbers_to_exclude = list(range(k)) # create a list that has the number from 0 to k-1\n",
        "list_of_cluster_numbers_to_exclude.remove(index_to_remove) # remove the cluster of leaf that we want to keep, and not black out\n",
        "for cluster in list_of_cluster_numbers_to_exclude:\n",
        "  masked_image[labels== cluster] = [0, 0, 0] # black all clusters except cluster leaf_center_index\n",
        "\n",
        "# convert back to original shape\n",
        "masked_image = masked_image.reshape(img.shape)\n",
        "masked_image_grayscale = rgb2gray(masked_image)\n",
        "\n",
        "# show the image\n",
        "plt.imshow(masked_image_grayscale, cmap=\"Greens\")\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "id": "RZ1Bd-y8F4rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count how many pixels are in the foreground and bg\n",
        "leaf_count = np.sum(np.array(masked_image_grayscale) >0)\n",
        "bg_count = np.sum(np.array(masked_image_grayscale) ==0)\n",
        "\n",
        "print('Leaf px count:', leaf_count, 'px')\n",
        "print('Area:', leaf_count*pixel_cm_ratio, 'cm\\N{SUPERSCRIPT TWO},', 'which is:',  f'{0.0001*leaf_count*pixel_cm_ratio:.3f}', 'm\\N{SUPERSCRIPT TWO}')"
      ],
      "metadata": {
        "id": "ibM-aSEGF8s7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create your first 'real' web-app, with streamlit (only Python!)"
      ],
      "metadata": {
        "id": "m6KPMPTydFpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libs\n",
        "import streamlit as st\n",
        "import cv2\n",
        "import numpy as np\n",
        "import skimage.io as io\n",
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "# check versions\n",
        "#np.__version__"
      ],
      "metadata": {
        "id": "qLmFiZoHdPam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to segment using k-means\n",
        "\n",
        "def segment_image_kmeans(img, k=3, attempts=10): \n",
        "\n",
        "    # Convert MxNx3 image into Kx3 where K=MxN\n",
        "    pixel_values  = img.reshape((-1,3))  #-1 reshape means, in this case MxN\n",
        "\n",
        "    #We convert the unit8 values to float as it is a requirement of the k-means method of OpenCV\n",
        "    pixel_values = np.float32(pixel_values)\n",
        "\n",
        "    # define stopping criteria\n",
        "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)\n",
        "    \n",
        "    _, labels, (centers) = cv2.kmeans(pixel_values, k, None, criteria, attempts, cv2.KMEANS_RANDOM_CENTERS)\n",
        "    \n",
        "    # convert back to 8 bit values\n",
        "    centers = np.uint8(centers)\n",
        "\n",
        "    # flatten the labels array\n",
        "    labels = labels.flatten()\n",
        "    \n",
        "    # convert all pixels to the color of the centroids\n",
        "    segmented_image = centers[labels.flatten()]\n",
        "    \n",
        "    # reshape back to the original image dimension\n",
        "    segmented_image = segmented_image.reshape(img.shape)\n",
        "    \n",
        "    return segmented_image"
      ],
      "metadata": {
        "id": "5B-hLFvYdT1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vars\n",
        "DEMO_IMAGE = 'demo.png' # a demo image for the segmentation page, if none is uploaded\n",
        "favicon = 'favicon.png'\n",
        "\n",
        "# main page\n",
        "st.set_page_config(page_title='K-Means - Yedidya Harris', page_icon = favicon, layout = 'wide', initial_sidebar_state = 'auto')\n",
        "st.title('Image Segmentation using K-Means, by Yedidya Harris')\n",
        "\n",
        "# side bar\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "    <style>\n",
        "    [data-testid=\"stSidebar\"][aria-expanded=\"true\"] . div:first-child{\n",
        "        width: 350px\n",
        "    }\n",
        "    \n",
        "    [data-testid=\"stSidebar\"][aria-expanded=\"false\"] . div:first-child{\n",
        "        width: 350px\n",
        "        margin-left: -350px\n",
        "    }    \n",
        "    </style>\n",
        "    \n",
        "    \"\"\",\n",
        "    unsafe_allow_html=True,\n",
        "\n",
        "\n",
        ")\n",
        "\n",
        "st.sidebar.title('Segmentation Sidebar')\n",
        "st.sidebar.subheader('Site Pages')"
      ],
      "metadata": {
        "id": "5Y2fB3fWdXoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using st.cache so streamlit runs the following function only once, and stores in chache (until changed)\n",
        "@st.cache()\n",
        "\n",
        "# take an image, and return a resized that fits our page\n",
        "def image_resize(image, width=None, height=None, inter = cv2.INTER_AREA):\n",
        "    dim = None\n",
        "    (h,w) = image.shape[:2]\n",
        "    \n",
        "    if width is None and height is None:\n",
        "        return image\n",
        "    \n",
        "    if width is None:\n",
        "        r = width/float(w)\n",
        "        dim = (int(w*r),height)\n",
        "    \n",
        "    else:\n",
        "        r = width/float(w)\n",
        "        dim = (width, int(h*r))\n",
        "        \n",
        "    # resize the image\n",
        "    resized = cv2.resize(image, dim, interpolation=inter)\n",
        "    \n",
        "    return resized"
      ],
      "metadata": {
        "id": "xX6c0uWidZ7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add dropdown to select pages on left\n",
        "app_mode = st.sidebar.selectbox('Navigate',\n",
        "                                  ['About App', 'Segment an Image'])"
      ],
      "metadata": {
        "id": "HvX9y1O2dcq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# About page\n",
        "if app_mode == 'About App':\n",
        "    st.markdown('In this app we will segment images using K-Means')\n",
        "    \n",
        "    \n",
        "    # side bar\n",
        "    st.markdown(\n",
        "        \"\"\"\n",
        "        <style>\n",
        "        [data-testid=\"stSidebar\"][aria-expanded=\"true\"] . div:first-child{\n",
        "            width: 350px\n",
        "        }\n",
        "\n",
        "        [data-testid=\"stSidebar\"][aria-expanded=\"false\"] . div:first-child{\n",
        "            width: 350px\n",
        "            margin-left: -350px\n",
        "        }    \n",
        "        </style>\n",
        "\n",
        "        \"\"\",\n",
        "        unsafe_allow_html=True,\n",
        "\n",
        "\n",
        "    )\n",
        "\n",
        "    # add a video to the page\n",
        "    st.video('https://www.youtube.com/watch?v=6CqRnx6Ic48')\n",
        "\n",
        "\n",
        "    st.markdown('''\n",
        "                ## About the app \\n\n",
        "                Hey, this web app is a great one to segment images using K-Means. \\n\n",
        "                There are many way. \\n\n",
        "                Enjoy! Yedidya\n",
        "\n",
        "\n",
        "                ''') "
      ],
      "metadata": {
        "id": "vb-zgciWdgZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run image\n",
        "if app_mode == 'Segment an Image':\n",
        "    \n",
        "    st.sidebar.markdown('---') # adds a devider (a line)\n",
        "    \n",
        "    # side bar\n",
        "    st.markdown(\n",
        "        \"\"\"\n",
        "        <style>\n",
        "        [data-testid=\"stSidebar\"][aria-expanded=\"true\"] . div:first-child{\n",
        "            width: 350px\n",
        "        }\n",
        "\n",
        "        [data-testid=\"stSidebar\"][aria-expanded=\"false\"] . div:first-child{\n",
        "            width: 350px\n",
        "            margin-left: -350px\n",
        "        }    \n",
        "        </style>\n",
        "\n",
        "        \"\"\",\n",
        "        unsafe_allow_html=True,\n",
        "\n",
        "\n",
        "    )\n",
        "\n",
        "    # choosing a k value (either with +- or with a slider)\n",
        "    k_value = st.sidebar.number_input('Insert K value (number of clusters):', value=4, min_value = 1) # asks for input from the user\n",
        "    st.sidebar.markdown('---') # adds a devider (a line)\n",
        "    \n",
        "    attempts_value_slider = st.sidebar.slider('Number of attempts', value = 7, min_value = 1, max_value = 10) # slider example\n",
        "    st.sidebar.markdown('---') # adds a devider (a line)\n",
        "    \n",
        "    # read an image from the user\n",
        "    img_file_buffer = st.sidebar.file_uploader(\"Upload an image\", type=['jpg', 'jpeg', 'png'])\n",
        "\n",
        "    # assign the uplodaed image from the buffer, by reading it in\n",
        "    if img_file_buffer is not None:\n",
        "        image = io.imread(img_file_buffer)\n",
        "    else: # if no image was uploaded, then segment the demo image\n",
        "        demo_image = DEMO_IMAGE\n",
        "        image = io.imread(demo_image)\n",
        "\n",
        "    # display on the sidebar the uploaded image\n",
        "    st.sidebar.text('Original Image')\n",
        "    st.sidebar.image(image)\n",
        "    \n",
        "    # call the function to segment the image\n",
        "    segmented_image = segment_image_kmeans(image, k=k_value, attempts=attempts_value_slider)\n",
        "    \n",
        "    # Display the result on the right (main frame)\n",
        "    st.subheader('Output Image')\n",
        "    st.image(segmented_image, use_column_width=True)"
      ],
      "metadata": {
        "id": "Y0cgz0jDdjLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first remove the markdown cells (because it causes problems in streamlit app)\n",
        "\n",
        "import nbformat as nbf\n",
        "folder_path =r'/content/drive/MyDrive/71254_2023/01_Lectures/Class07/kmeans_webapp_yHarris'\n",
        "ntbk_name_to_convert = 'kmeans.ipynb' # find it in your drive, or upload it to the content\n",
        "ntbk = nbf.read(f'{folder_path}/kmeans.ipynb', nbf.NO_CONVERT)\n",
        "new_ntbk = ntbk\n",
        "new_ntbk.cells = [cell for cell in ntbk.cells if cell.cell_type != \"markdown\"]\n",
        "nbf.write(new_ntbk, f'new_{ntbk_name_to_convert}', version=nbf.NO_CONVERT)"
      ],
      "metadata": {
        "id": "23a5tQaJdosR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# then convert to .py format\n",
        "!pip install ipython\n",
        "!pip install nbconvert\n",
        "\n",
        "# the conversion (it'll save the .py file on the left, in the content)\n",
        "!ipython nbconvert new_kmeans.ipynb --to python"
      ],
      "metadata": {
        "id": "zEYCF-b2dq8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add more libraries if you used! as a new line\n",
        "with open('requirements.txt', 'w') as f:\n",
        "    f.write('''streamlit\n",
        "scikit-image\n",
        "opencv-contrib-python-headless\n",
        "numpy''')"
      ],
      "metadata": {
        "id": "-nTgyWNJdtU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#class 9"
      ],
      "metadata": {
        "id": "UE7WOPhvkBGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geemap"
      ],
      "metadata": {
        "id": "g2p36oBS7D7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ee\n",
        "\n",
        "# Trigger the authentication flow.\n",
        "ee.Authenticate()\n",
        "\n",
        "# Initialize the library.\n",
        "ee.Initialize()"
      ],
      "metadata": {
        "id": "PZxyfPUF7En-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import geemap\n",
        "import pandas as pd\n",
        "from IPython.display import Image\n",
        "import ee, datetime\n",
        "from pylab import *\n",
        "from matplotlib.pylab import rcParams"
      ],
      "metadata": {
        "id": "vHTSMfiq7HVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load an image of LANDSAT 8 RGB\n",
        "map1 = geemap.Map()\n",
        "image = ee.Image('LANDSAT/LC08/C01/T1_SR/LC08_044034_20140318')\n",
        "# Center the map and display the image.\n",
        "map1.centerObject(image, zoom=8)\n",
        "vis_params = {\n",
        "    'bands': ['B4', 'B3', 'B2'],\n",
        "    'min': 0.0,\n",
        "    'max': 3000,\n",
        "    'opacity': 1.0,\n",
        "    'gamma': 1.2,\n",
        "}\n",
        "map1.addLayer(image, vis_params, \"Landsat Vis\")\n",
        "map1"
      ],
      "metadata": {
        "id": "G1HoR7bh7K2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get info on the image\n",
        "image = ee.Image('LANDSAT/LC08/C01/T1_SR/LC08_044034_20140318')\n",
        "props = geemap.image_props(image)\n",
        "props.getInfo()#call the data to our pc"
      ],
      "metadata": {
        "id": "jkjGRfl57NA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a Landsat 8 image-do NDVI\n",
        "image = ee.Image('LANDSAT/LC08/C01/T1_TOA/LC08_044034_20140318')\n",
        "# Compute the NDVI using an expression.\n",
        "NDVI = image.expression(\n",
        "    ' ((NIR - RED) / (NIR + RED))', {\n",
        "      'NIR': image.select('B5'),\n",
        "      'RED': image.select('B4'),\n",
        "}).rename('NDVI');\n",
        "palette =  ['FFFFFF', 'CE7E45', 'DF923D', 'F1B555', 'FCD163', '99B718',\n",
        "               '74A901', '66A000', '529400', '3E8601', '207401', '056201',\n",
        "               '004C00', '023B01', '012E01', '011D01', '011301']\n",
        "map1.centerObject(image, 9)\n",
        "map1.addLayer(NDVI, {'min': 0, 'max': 1, 'palette':palette}, 'NDVI')\n",
        "map1"
      ],
      "metadata": {
        "id": "8BNSkssU7O8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del(map1)"
      ],
      "metadata": {
        "id": "8cFgKAxc7STC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sentinel 2 image israel\n",
        "# initialize our map\n",
        "map1 = geemap.Map()\n",
        "AOI = ee.Geometry.Point(35.054342198020024, 31.35047863722117)\n",
        "se2 = ee.ImageCollection('COPERNICUS/S2').filterDate(\"2019-01-01\",\"2019-12-31\").filterBounds(AOI).first()\n",
        "#False color imagery is displayed in a combination of standard near \n",
        "# infra-red, red and green band. False color composite using near infrared,\n",
        "# red and green bands is very popular. It is most commonly used to assess plant density\n",
        "# and healht, as plants reflect near infrared and green light, while absorbing red. \n",
        "# Since they reflect more near infrared than green, plant-covered land appears deep\n",
        "#  red. Denser plant growth is darker red.\n",
        "# Cities and exposed ground are gray or tan, and water appears blue or black.\n",
        "rgb = ['B8','B4','B3']\n",
        "# set some thresholds\n",
        "rgbViz = {\"min\":0.0, \"max\":3000,\"bands\":rgb}\n",
        "# initialize our map\n",
        "map1.centerObject(AOI, 7)\n",
        "map1.addLayer(se2, rgbViz, \"S2\")\n",
        "map1.addLayerControl()\n",
        "map1"
      ],
      "metadata": {
        "id": "sqkW4Qop7U78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://medium.com/@melqkiades/water-detection-using-ndwi-on-google-earth-engine-2919a9bf1951\n",
        "#https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/ndwi/\n",
        "NDWI = se2.normalizedDifference([ 'B3','B8',]).rename('NDWI')\n",
        "palette =  ['red', 'yellow', 'green', 'cyan', 'blue']\n",
        "map1.centerObject(se2, 9)\n",
        "map1.addLayer(NDWI, {'min': -1, 'max': 1, 'palette':palette}, 'NDWI')\n",
        "map1"
      ],
      "metadata": {
        "id": "ihu4m8L97YHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://medium.com/@melqkiades/water-detection-using-ndwi-on-google-earth-engine-2919a9bf1951\n",
        "#https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/ndwi/\n",
        "NDWI = se2.normalizedDifference([ 'B3','B8',]).rename('NDWI')\n",
        "NDWIThreshold = NDWI.gte(0.0);\n",
        "NDWIMask = NDWIThreshold.updateMask(NDWIThreshold);\n",
        "palette =  ['blue']\n",
        "map1.centerObject(se2, 9)\n",
        "map1.addLayer(NDWIMask,  {'min': 0, 'max': 1, 'palette':palette}, 'NDWI MASK')\n",
        "map1"
      ],
      "metadata": {
        "id": "-f0NdN1R7asr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del(map1)"
      ],
      "metadata": {
        "id": "vjUBu83f7dBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sentinel 2 image israel\n",
        "AOI = ee.Geometry.Point(35.054342198020024, 31.35047863722117)\n",
        "se2 = ee.ImageCollection('COPERNICUS/S2').filterDate(\"2019-01-01\",\"2019-12-31\").filterBounds(AOI).first()\n",
        "# set some thresholds\n",
        "rgb = ['B4','B3','B2']\n",
        "# set some thresholds\n",
        "rgbViz = {\"min\":0.0, \"max\":3000,\"bands\":rgb}\n",
        "# initialize our map\n",
        "     \n",
        "\n",
        "# initialize our map\n",
        "map1 = geemap.Map()\n",
        "map1.centerObject(AOI, 7)\n",
        "map1.addLayer(se2, rgbViz, \"S2\")\n",
        "map1.addLayerControl()\n",
        "map1"
      ],
      "metadata": {
        "id": "XoEP8xbI7qYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NDVI = se2.expression(\n",
        "    ' ((NIR - RED) / (NIR + RED))', {\n",
        "      'NIR': se2.select('B8'),\n",
        "      'RED': se2.select('B4'),}).rename('NDVI');\n",
        "\n",
        "\n",
        "\n",
        "palette =  ['FFFFFF', 'CE7E45', 'DF923D', 'F1B555', 'FCD163', '99B718',\n",
        "               '74A901', '66A000', '529400', '3E8601', '207401', '056201',\n",
        "               '004C00', '023B01', '012E01', '011D01', '011301']\n",
        "map1.centerObject(se2, 9)\n",
        "map1.addLayer(NDVI, {'min': 0, 'max': 1, 'palette':palette}, 'NDVI')\n",
        "map1"
      ],
      "metadata": {
        "id": "ybCf9c317rCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get December image, we're using the \"avg_rad\" band\n",
        "\n",
        "#se2 = ee.ImageCollection('COPERNICUS/S2').filterDate(\"2020-05-01\",\"2022-12-31\").filterBounds(geometry)#.first()\n",
        "my_boundary = ee.Geometry.Polygon([ [[35.13008776970655, 31.288173501430748],[35.13008776970655, 31.261764527107875],[35.16304675408155, 31.261764527107875],[35.16304675408155, 31.288173501430748]]] ,proj=None)\n",
        "#fc = ee.FeatureCollection([ee.Feature(ee.Geometry.Polygon([[-109.05, 41], [-109.05, 37], [-102.05, 37], [-102.05, 41])\n",
        "#viirs2017_12 = ee.ImageCollection(\"NOAA/VIIRS/DNB/MONTHLY_V1/VCMSLCFG\").filterDate(\"2017-12-01\",\"2017-12-31\").select('avg_rad').first()\n",
        "#polygon2 = ee.Geometry.Polygon([\n",
        "#  [[-122.06197,37.04748], [-122.1830,37.4748], [-122.1830,37.8032], [-122.6197,37.8032], [-122.061974,37.047484]]]);\n",
        "tls = ee.Feature(ee.FeatureCollection(my_boundary).geometry())\n",
        "\n",
        "NDVI_clip = NDVI.clip(tls)\n",
        "map1 = geemap.Map()\n",
        "map1.centerObject(NDVI_clip, zoom=8)\n",
        "map1.add_basemap('SATELLITE')\n",
        "#map1.addLayer(X.select('NDVI'), {})\n",
        "map1.addLayer(NDVI_clip.select('NDVI'), {'min': 0, 'max': 1, 'palette':palette}, 'NDVI')\n",
        "#Map.addLayer(X, {'min': 0, 'max': 1, 'palette':palette}, 'NDVI')\n",
        "map1.addLayerControl()\n",
        "map1"
      ],
      "metadata": {
        "id": "fbkC7xkc7tfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "reduced = NDVI_clip.reduceRegion(\n",
        "              reducer=ee.Reducer.mean(),\n",
        "              geometry=my_boundary,\n",
        "              scale=10)\n",
        "print(reduced.get('NDVI').getInfo())\n"
      ],
      "metadata": {
        "id": "bdxZ0vv87wJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export the image, specifying scale and region.\n",
        "task = ee.batch.Export.image.toDrive(**{\n",
        "    'image': NDVI_clip,\n",
        "    'description': 'imageToDriveExample',\n",
        "    'folder':'Example_folder',\n",
        "    'scale': 10,\n",
        "    'region': my_boundary.getInfo()['coordinates']\n",
        "})\n",
        "task.start()"
      ],
      "metadata": {
        "id": "bRe1PqSp8DYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import rasterio\n",
        "dataset = rasterio.open('/content/imageToDriveExample.tif')"
      ],
      "metadata": {
        "id": "TxOcWnoF8EHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.crs\n",
        "dataset.profile\n",
        "from rasterio.plot import show\n",
        "show(dataset)"
      ],
      "metadata": {
        "id": "47cqH4bg8I8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#more data not satiile image\n",
        "map1 = geemap.Map()\n",
        "palette = cm.palettes.dem\n",
        "# palette = cm.palettes.terrain\n",
        "dem = ee.Image('USGS/SRTMGL1_003')\n",
        "vis_params = {'min': 0, 'max': 4000, 'palette': palette}\n",
        "map1.addLayer(dem, vis_params, 'SRTM DEM')\n",
        "map1.add_colorbar(vis_params, label=\"Elevation (m)\", layer_name=\"SRTM DEM\")\n",
        "map1"
      ],
      "metadata": {
        "id": "ARX759ij8Pkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#not sure we need\n",
        "import geemap.colormaps as cm\n",
        "map1 = geemap.Map()\n",
        "palette = cm.palettes.ocean_r\n",
        "# palette = cm.palettes.terrain\n",
        "dataset = ee.ImageCollection('UCSB-CHG/CHIRPS/PENTAD').filter(ee.Filter.date('2018-05-01', '2018-05-05'))  \n",
        "#dataset = ee.ImageCollection('UCSB-CHG/CHIRPS/PENTAD').filter(ee.Filter.date('2018-01-01', '2019-01-01')).max()#we can also do sum (:  \n",
        "precipitation = dataset.select('precipitation')#.first()\n",
        "vis_params = {'min': 0, 'max': 1072, 'palette': palette}\n",
        "map1.addLayer(precipitation, vis_params, 'palette')\n",
        "map1.add_colorbar(vis_params, label=\"precipitation (mm/pentad)\", layer_name=\"precipitation\")\n",
        "\n",
        "map1"
      ],
      "metadata": {
        "id": "0ptGhCrw8XS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sentinel 2 image israel\n",
        "AOI = ee.Geometry.Point(35.570035433516296, 33.0444322996468)\n",
        "se2 = ee.ImageCollection('COPERNICUS/S2').filterDate('2019-09-01','2020-11-01').filterBounds(AOI).first()\n",
        "#se2 = ee.ImageCollection('COPERNICUS/S2').filterDate('2019-06-01','2020-11-01').filterBounds(AOI).first()#with cloud\n",
        "\n",
        "# set some thresholds\n",
        "rgb = ['B4','B3','B2']\n",
        "# set some thresholds\n",
        "rgbViz = {\"min\":0.0, \"max\":3000,\"bands\":rgb}\n",
        "# initialize our map\n",
        "     \n",
        "\n",
        "# initialize our map\n",
        "map1 = geemap.Map()\n",
        "map1.centerObject(AOI, 7)\n",
        "map1.addLayer(se2, rgbViz, \"S2\")\n",
        "map1.addLayerControl()\n",
        "map1"
      ],
      "metadata": {
        "id": "ndceLqaF8YCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AOI = ee.Geometry.Point(35.570035433516296, 33.0444322996468)\n",
        "start_date = '2019-06-01'\n",
        "end_date = '2020-09-01'\n",
        "\n",
        "s2_sr_no_cloud_mask = ee.ImageCollection('COPERNICUS/S2_SR').filterBounds(AOI)  .filterDate(start_date, end_date)\n",
        "      \n",
        "        "
      ],
      "metadata": {
        "id": "AXpO6jIz8ctZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import datetime\n",
        "#35.62093419073687, 33.10805948139925 crop\n",
        "#35.609261217104056, 33.101876261159404 swemp\n",
        "point = {'type':'Point', 'coordinates':[ 35.62093419073687, 33.10805948139925]};\n",
        "#point = {'type':'Point', 'coordinates':[31.35047863722117, 35.054342198020024]};\n",
        "info = s2_sr_no_cloud_mask.getRegion(point,500).getInfo()\n",
        "# Reshape image collection \n",
        "header = info[0]\n",
        "data = np.array(info[1:])\n",
        "data"
      ],
      "metadata": {
        "id": "IA7ERjj88VQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape image collection \n",
        "header = info[0]\n",
        "data = np.array(info[1:])\n",
        "\n",
        "iTime = header.index('time')\n",
        "time = [datetime.datetime.fromtimestamp(i/1000) for i in (data[0:,iTime].astype(int))]\n",
        "\n",
        "# List of used image bands\n",
        "#band_list = ['B4',u'B8']\n",
        "band_list = ['B4','B8']\n",
        "iBands = [header.index(b) for b in band_list]\n",
        "yData = data[0:,iBands].astype(np.float)\n",
        "\n",
        "# Calculate NDVI\n",
        "red = yData[:,0]\n",
        "nir = yData[:,1]\n",
        "ndvi = (nir - red) / (nir + red)"
      ],
      "metadata": {
        "id": "KYLdM4sY8nG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(data=ndvi, index=list(range(len(ndvi))), columns=['NDVI'])\n",
        "df = df.interpolate()\n",
        "df['Date'] = pd.Series(time, index=df.index)"
      ],
      "metadata": {
        "id": "IX7604FX8pUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rcParams['figure.figsize'] = 15, 6\n",
        "df.plot(y='NDVI',x='Date')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YPoG3nIG8rmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#AOI = ee.Geometry.Point(-122.269, 45.701)\n",
        "AOI = ee.Geometry.Point(35.570035433516296, 33.0444322996468)\n",
        "start_date = '2019-06-01'\n",
        "end_date = '2020-09-01'\n",
        "CLOUD_FILTER = 60\n",
        "CLD_PRB_THRESH = 40\n",
        "NIR_DRK_THRESH = 0.15\n",
        "CLD_PRJ_DIST = 2\n",
        "BUFFER = 50\n"
      ],
      "metadata": {
        "id": "tkEbf6jU8tzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_s2_sr_cld_col(aoi, start_date, end_date):\n",
        "    # Import and filter S2 SR.\n",
        "    s2_sr_col = (ee.ImageCollection('COPERNICUS/S2_SR')\n",
        "        .filterBounds(aoi)\n",
        "        .filterDate(start_date, end_date)\n",
        "        .filter(ee.Filter.lte('CLOUDY_PIXEL_PERCENTAGE', CLOUD_FILTER)))\n",
        "\n",
        "    # Import and filter s2cloudless.\n",
        "    s2_cloudless_col = (ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n",
        "        .filterBounds(aoi)\n",
        "        .filterDate(start_date, end_date))\n",
        "\n",
        "    # Join the filtered s2cloudless collection to the SR collection by the 'system:index' property.\n",
        "    return ee.ImageCollection(ee.Join.saveFirst('s2cloudless').apply(**{\n",
        "        'primary': s2_sr_col,\n",
        "        'secondary': s2_cloudless_col,\n",
        "        'condition': ee.Filter.equals(**{\n",
        "            'leftField': 'system:index',\n",
        "            'rightField': 'system:index'\n",
        "        })\n",
        "    }))\n",
        "#Apply the `get_s2_sr_cld_col` function to build a collection according to the parameters defined above.\n",
        "START_DATE =start_date\n",
        "END_DATE =end_date\n",
        "s2_sr_cld_col_eval = get_s2_sr_cld_col(AOI, START_DATE, END_DATE)\n",
        "\n",
        "# Cloud components\n",
        "\n",
        "#Define a function to add the s2cloudless probability layer and derived cloud mask as bands to an S2 SR image input.\n",
        "\n",
        "def add_cloud_bands(img):\n",
        "    # Get s2cloudless image, subset the probability band.\n",
        "    cld_prb = ee.Image(img.get('s2cloudless')).select('probability')\n",
        "\n",
        "    # Condition s2cloudless by the probability threshold value.\n",
        "    is_cloud = cld_prb.gt(CLD_PRB_THRESH).rename('clouds')\n",
        "\n",
        "    # Add the cloud probability layer and cloud mask as image bands.\n",
        "    return img.addBands(ee.Image([cld_prb, is_cloud]))\n",
        "#### Cloud shadow components\n",
        "\n",
        "#Define a function to add dark pixels, cloud projection, and identified \n",
        "#shadows as bands to an S2 SR image input. Note that the image input needs to be the result of the above `add_cloud_bands` \n",
        "#function because it relies on knowing which pixels are considered cloudy (`'clouds'` band)\n",
        "def add_shadow_bands(img):\n",
        "    # Identify water pixels from the SCL band.\n",
        "    not_water = img.select('SCL').neq(6)\n",
        "\n",
        "    # Identify dark NIR pixels that are not water (potential cloud shadow pixels).\n",
        "    SR_BAND_SCALE = 1e4\n",
        "    dark_pixels = img.select('B8').lt(NIR_DRK_THRESH*SR_BAND_SCALE).multiply(not_water).rename('dark_pixels')\n",
        "\n",
        "    # Determine the direction to project cloud shadow from clouds (assumes UTM projection).\n",
        "    shadow_azimuth = ee.Number(90).subtract(ee.Number(img.get('MEAN_SOLAR_AZIMUTH_ANGLE')));\n",
        "\n",
        "    # Project shadows from clouds for the distance specified by the CLD_PRJ_DIST input.\n",
        "    cld_proj = (img.select('clouds').directionalDistanceTransform(shadow_azimuth, CLD_PRJ_DIST*10)\n",
        "        .reproject(**{'crs': img.select(0).projection(), 'scale': 100})\n",
        "        .select('distance')\n",
        "        .mask()\n",
        "        .rename('cloud_transform'))\n",
        "\n",
        "    # Identify the intersection of dark pixels with cloud shadow projection.\n",
        "    shadows = cld_proj.multiply(dark_pixels).rename('shadows')\n",
        "\n",
        "    # Add dark pixels, cloud projection, and identified shadows as image bands.\n",
        "    return img.addBands(ee.Image([dark_pixels, cld_proj, shadows]))\n",
        "def add_cld_shdw_mask(img):\n",
        "    # Add cloud component bands.\n",
        "    img_cloud = add_cloud_bands(img)\n",
        "\n",
        "    # Add cloud shadow component bands.\n",
        "    img_cloud_shadow = add_shadow_bands(img_cloud)\n",
        "\n",
        "    # Combine cloud and shadow mask, set cloud and shadow as value 1, else 0.\n",
        "    is_cld_shdw = img_cloud_shadow.select('clouds').add(img_cloud_shadow.select('shadows')).gt(0)\n",
        "\n",
        "    # Remove small cloud-shadow patches and dilate remaining pixels by BUFFER input.\n",
        "    # 20 m scale is for speed, and assumes clouds don't require 10 m precision.\n",
        "    is_cld_shdw = (is_cld_shdw.focalMin(2).focalMax(BUFFER*2/20)\n",
        "        .reproject(**{'crs': img.select([0]).projection(), 'scale': 20})\n",
        "        .rename('cloudmask'))\n",
        "\n",
        "    # Add the final cloud-shadow mask to the image.\n",
        "    return img_cloud_shadow.addBands(is_cld_shdw)"
      ],
      "metadata": {
        "id": "D_D6-xj-86oX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_cld_shdw_mask(img):\n",
        "    # Subset the cloudmask band and invert it so clouds/shadow are 0, else 1.\n",
        "    not_cld_shdw = img.select('cloudmask').Not()\n",
        "\n",
        "    # Subset reflectance bands and update their masks, return the result.\n",
        "    return img.select('B.*').updateMask(not_cld_shdw)"
      ],
      "metadata": {
        "id": "tSAciKuL8-er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s2_sr = (s2_sr_cld_col.map(add_cld_shdw_mask).map(apply_cld_shdw_mask))"
      ],
      "metadata": {
        "id": "CMivZNEm9Anb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import datetime\n",
        "#35.62093419073687, 33.10805948139925 crop\n",
        "#35.609261217104056, 33.101876261159404 swemp\n",
        "point = {'type':'Point', 'coordinates':[ 35.62093419073687, 33.10805948139925]};\n",
        "#point = {'type':'Point', 'coordinates':[31.35047863722117, 35.054342198020024]};\n",
        "info = s2_sr.getRegion(point,10).getInfo()\n",
        "# Reshape image collection \n",
        "header = info[0]\n",
        "data = np.array(info[1:])\n",
        "data"
      ],
      "metadata": {
        "id": "cwATBHZz9C_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape image collection \n",
        "header = info[0]\n",
        "data = np.array(info[1:])\n",
        "\n",
        "iTime = header.index('time')\n",
        "time = [datetime.datetime.fromtimestamp(i/1000) for i in (data[0:,iTime].astype(int))]\n",
        "\n",
        "# List of used image bands\n",
        "#band_list = ['B4',u'B8']\n",
        "band_list = ['B4','B8']\n",
        "iBands = [header.index(b) for b in band_list]\n",
        "yData = data[0:,iBands].astype(np.float)\n",
        "\n",
        "# Calculate NDVI\n",
        "red = yData[:,0]\n",
        "nir = yData[:,1]\n",
        "ndvi = (nir - red) / (nir + red)"
      ],
      "metadata": {
        "id": "ynsSsqwm9Frq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(data=ndvi, index=list(range(len(ndvi))), columns=['NDVI'])\n",
        "df = df.interpolate()\n",
        "df['Date'] = pd.Series(time, index=df.index)"
      ],
      "metadata": {
        "id": "-gSnGlvW9IAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rcParams['figure.figsize'] = 15, 6\n",
        "df.plot(y='NDVI',x='Date')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w6ZkFLFu9Kt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Map = geemap.Map()\n",
        "# Load an image.\n",
        "# Center the map and display the image.\n",
        "vis_params = {\n",
        "    'bands': ['B4', 'B3', 'B2'],\n",
        "    'min': 0.0,\n",
        "    'max': 3000,\n",
        "    'opacity': 1,\n",
        "    'gamma': 1.2,\n",
        "}\n",
        "Map.addLayer(s2_sr_median, vis_params, \"Vis\")\n",
        "Map\n",
        "#its wil take time to load"
      ],
      "metadata": {
        "id": "LY7he8TB9ODQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#class 10"
      ],
      "metadata": {
        "id": "s5cRBBMEkCmT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image Classification with TensorFlow Hub\n",
        "\n",
        "In this colab, you'll try multiple image classification models from TensorFlow Hub and decide which one is best for your use case.\n",
        "\n",
        "Because TF Hub encourages a [consistent input convention](https://www.tensorflow.org/hub/common_saved_model_apis/images#image_input) for models that operate on images, it's easy to experiment with different architectures to find the one that best fits your needs."
      ],
      "metadata": {
        "id": "SPVv0d2N9qqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import libs\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "W8QLNiE-9r1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper functions for loading image (hidden)\n",
        "\n",
        "original_image_cache = {}\n",
        "\n",
        "def preprocess_image(image):\n",
        "  image = np.array(image)\n",
        "  # reshape into shape [batch_size, height, width, num_channels]\n",
        "  img_reshaped = tf.reshape(image, [1, image.shape[0], image.shape[1], image.shape[2]])\n",
        "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
        "  image = tf.image.convert_image_dtype(img_reshaped, tf.float32)\n",
        "  return image\n",
        "\n",
        "def load_image_from_url(img_url):\n",
        "  \"\"\"Returns an image with shape [1, height, width, num_channels].\"\"\"\n",
        "  user_agent = {'User-agent': 'Colab Sample (https://tensorflow.org)'}\n",
        "  response = requests.get(img_url, headers=user_agent)\n",
        "  image = Image.open(BytesIO(response.content))\n",
        "  image = preprocess_image(image)\n",
        "  return image\n",
        "\n",
        "def load_image(image_url, image_size=256, dynamic_size=False, max_dynamic_size=512):\n",
        "  \"\"\"Loads and preprocesses images.\"\"\"\n",
        "  # Cache image file locally.\n",
        "  if image_url in original_image_cache:\n",
        "    img = original_image_cache[image_url]\n",
        "  elif image_url.startswith('https://'):\n",
        "    img = load_image_from_url(image_url)\n",
        "  else:\n",
        "    fd = tf.io.gfile.GFile(image_url, 'rb')\n",
        "    img = preprocess_image(Image.open(fd))\n",
        "  original_image_cache[image_url] = img\n",
        "  # Load and convert to float32 numpy array, add batch dimension, and normalize to range [0, 1].\n",
        "  img_raw = img\n",
        "  if tf.reduce_max(img) > 1.0:\n",
        "    img = img / 255.\n",
        "  if len(img.shape) == 3:\n",
        "    img = tf.stack([img, img, img], axis=-1)\n",
        "  if not dynamic_size:\n",
        "    img = tf.image.resize_with_pad(img, image_size, image_size)\n",
        "  elif img.shape[1] > max_dynamic_size or img.shape[2] > max_dynamic_size:\n",
        "    img = tf.image.resize_with_pad(img, max_dynamic_size, max_dynamic_size)\n",
        "  return img, img_raw\n",
        "\n",
        "def show_image(image, title=''):\n",
        "  image_size = image.shape[1]\n",
        "  w = (image_size * 6) // 320\n",
        "  plt.figure(figsize=(w, w))\n",
        "  plt.imshow(image[0], aspect='equal')\n",
        "  plt.axis('off')\n",
        "  plt.title(title)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Xb2WMHyn9uBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Select an Image Classification model\n",
        "\n",
        "image_size = 224\n",
        "dynamic_size = False\n",
        "\n",
        "model_name = \"resnet_v2_152\" # @param ['efficientnetv2-s', 'efficientnetv2-m', 'efficientnetv2-l', 'efficientnetv2-s-21k', 'efficientnetv2-m-21k', 'efficientnetv2-l-21k', 'efficientnetv2-xl-21k', 'efficientnetv2-b0-21k', 'efficientnetv2-b1-21k', 'efficientnetv2-b2-21k', 'efficientnetv2-b3-21k', 'efficientnetv2-s-21k-ft1k', 'efficientnetv2-m-21k-ft1k', 'efficientnetv2-l-21k-ft1k', 'efficientnetv2-xl-21k-ft1k', 'efficientnetv2-b0-21k-ft1k', 'efficientnetv2-b1-21k-ft1k', 'efficientnetv2-b2-21k-ft1k', 'efficientnetv2-b3-21k-ft1k', 'efficientnetv2-b0', 'efficientnetv2-b1', 'efficientnetv2-b2', 'efficientnetv2-b3', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'bit_s-r50x1', 'inception_v3', 'inception_resnet_v2', 'resnet_v1_50', 'resnet_v1_101', 'resnet_v1_152', 'resnet_v2_50', 'resnet_v2_101', 'resnet_v2_152', 'nasnet_large', 'nasnet_mobile', 'pnasnet_large', 'mobilenet_v2_100_224', 'mobilenet_v2_130_224', 'mobilenet_v2_140_224', 'mobilenet_v3_small_100_224', 'mobilenet_v3_small_075_224', 'mobilenet_v3_large_100_224', 'mobilenet_v3_large_075_224']\n",
        "\n",
        "model_handle_map = {\n",
        "  \"efficientnetv2-s\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/classification/2\",\n",
        "  \"efficientnetv2-m\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_m/classification/2\",\n",
        "  \"efficientnetv2-l\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_l/classification/2\",\n",
        "  \"efficientnetv2-s-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_s/classification/2\",\n",
        "  \"efficientnetv2-m-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_m/classification/2\",\n",
        "  \"efficientnetv2-l-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_l/classification/2\",\n",
        "  \"efficientnetv2-xl-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_xl/classification/2\",\n",
        "  \"efficientnetv2-b0-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b0/classification/2\",\n",
        "  \"efficientnetv2-b1-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b1/classification/2\",\n",
        "  \"efficientnetv2-b2-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b2/classification/2\",\n",
        "  \"efficientnetv2-b3-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b3/classification/2\",\n",
        "  \"efficientnetv2-s-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_s/classification/2\",\n",
        "  \"efficientnetv2-m-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_m/classification/2\",\n",
        "  \"efficientnetv2-l-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_l/classification/2\",\n",
        "  \"efficientnetv2-xl-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_xl/classification/2\",\n",
        "  \"efficientnetv2-b0-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b0/classification/2\",\n",
        "  \"efficientnetv2-b1-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b1/classification/2\",\n",
        "  \"efficientnetv2-b2-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b2/classification/2\",\n",
        "  \"efficientnetv2-b3-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b3/classification/2\",\n",
        "  \"efficientnetv2-b0\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/classification/2\",\n",
        "  \"efficientnetv2-b1\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b1/classification/2\",\n",
        "  \"efficientnetv2-b2\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b2/classification/2\",\n",
        "  \"efficientnetv2-b3\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b3/classification/2\",\n",
        "  \"efficientnet_b0\": \"https://tfhub.dev/tensorflow/efficientnet/b0/classification/1\",\n",
        "  \"efficientnet_b1\": \"https://tfhub.dev/tensorflow/efficientnet/b1/classification/1\",\n",
        "  \"efficientnet_b2\": \"https://tfhub.dev/tensorflow/efficientnet/b2/classification/1\",\n",
        "  \"efficientnet_b3\": \"https://tfhub.dev/tensorflow/efficientnet/b3/classification/1\",\n",
        "  \"efficientnet_b4\": \"https://tfhub.dev/tensorflow/efficientnet/b4/classification/1\",\n",
        "  \"efficientnet_b5\": \"https://tfhub.dev/tensorflow/efficientnet/b5/classification/1\",\n",
        "  \"efficientnet_b6\": \"https://tfhub.dev/tensorflow/efficientnet/b6/classification/1\",\n",
        "  \"efficientnet_b7\": \"https://tfhub.dev/tensorflow/efficientnet/b7/classification/1\",\n",
        "  \"bit_s-r50x1\": \"https://tfhub.dev/google/bit/s-r50x1/ilsvrc2012_classification/1\",\n",
        "  \"inception_v3\": \"https://tfhub.dev/google/imagenet/inception_v3/classification/4\",\n",
        "  \"inception_resnet_v2\": \"https://tfhub.dev/google/imagenet/inception_resnet_v2/classification/4\",\n",
        "  \"resnet_v1_50\": \"https://tfhub.dev/google/imagenet/resnet_v1_50/classification/4\",\n",
        "  \"resnet_v1_101\": \"https://tfhub.dev/google/imagenet/resnet_v1_101/classification/4\",\n",
        "  \"resnet_v1_152\": \"https://tfhub.dev/google/imagenet/resnet_v1_152/classification/4\",\n",
        "  \"resnet_v2_50\": \"https://tfhub.dev/google/imagenet/resnet_v2_50/classification/4\",\n",
        "  \"resnet_v2_101\": \"https://tfhub.dev/google/imagenet/resnet_v2_101/classification/4\",\n",
        "  \"resnet_v2_152\": \"https://tfhub.dev/google/imagenet/resnet_v2_152/classification/4\",\n",
        "  \"nasnet_large\": \"https://tfhub.dev/google/imagenet/nasnet_large/classification/4\",\n",
        "  \"nasnet_mobile\": \"https://tfhub.dev/google/imagenet/nasnet_mobile/classification/4\",\n",
        "  \"pnasnet_large\": \"https://tfhub.dev/google/imagenet/pnasnet_large/classification/4\",\n",
        "  \"mobilenet_v2_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/classification/4\",\n",
        "  \"mobilenet_v2_130_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/4\",\n",
        "  \"mobilenet_v2_140_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/classification/4\",\n",
        "  \"mobilenet_v3_small_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/classification/5\",\n",
        "  \"mobilenet_v3_small_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_075_224/classification/5\",\n",
        "  \"mobilenet_v3_large_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/classification/5\",\n",
        "  \"mobilenet_v3_large_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_075_224/classification/5\",\n",
        "}\n",
        "\n",
        "model_image_size_map = {\n",
        "  \"efficientnetv2-s\": 384,\n",
        "  \"efficientnetv2-m\": 480,\n",
        "  \"efficientnetv2-l\": 480,\n",
        "  \"efficientnetv2-b0\": 224,\n",
        "  \"efficientnetv2-b1\": 240,\n",
        "  \"efficientnetv2-b2\": 260,\n",
        "  \"efficientnetv2-b3\": 300,\n",
        "  \"efficientnetv2-s-21k\": 384,\n",
        "  \"efficientnetv2-m-21k\": 480,\n",
        "  \"efficientnetv2-l-21k\": 480,\n",
        "  \"efficientnetv2-xl-21k\": 512,\n",
        "  \"efficientnetv2-b0-21k\": 224,\n",
        "  \"efficientnetv2-b1-21k\": 240,\n",
        "  \"efficientnetv2-b2-21k\": 260,\n",
        "  \"efficientnetv2-b3-21k\": 300,\n",
        "  \"efficientnetv2-s-21k-ft1k\": 384,\n",
        "  \"efficientnetv2-m-21k-ft1k\": 480,\n",
        "  \"efficientnetv2-l-21k-ft1k\": 480,\n",
        "  \"efficientnetv2-xl-21k-ft1k\": 512,\n",
        "  \"efficientnetv2-b0-21k-ft1k\": 224,\n",
        "  \"efficientnetv2-b1-21k-ft1k\": 240,\n",
        "  \"efficientnetv2-b2-21k-ft1k\": 260,\n",
        "  \"efficientnetv2-b3-21k-ft1k\": 300, \n",
        "  \"efficientnet_b0\": 224,\n",
        "  \"efficientnet_b1\": 240,\n",
        "  \"efficientnet_b2\": 260,\n",
        "  \"efficientnet_b3\": 300,\n",
        "  \"efficientnet_b4\": 380,\n",
        "  \"efficientnet_b5\": 456,\n",
        "  \"efficientnet_b6\": 528,\n",
        "  \"efficientnet_b7\": 600,\n",
        "  \"inception_v3\": 299,\n",
        "  \"inception_resnet_v2\": 299,\n",
        "  \"mobilenet_v2_100_224\": 224,\n",
        "  \"mobilenet_v2_130_224\": 224,\n",
        "  \"mobilenet_v2_140_224\": 224,\n",
        "  \"nasnet_large\": 331,\n",
        "  \"nasnet_mobile\": 224,\n",
        "  \"pnasnet_large\": 331,\n",
        "  \"resnet_v1_50\": 224,\n",
        "  \"resnet_v1_101\": 224,\n",
        "  \"resnet_v1_152\": 224,\n",
        "  \"resnet_v2_50\": 224,\n",
        "  \"resnet_v2_101\": 224,\n",
        "  \"resnet_v2_152\": 224,\n",
        "  \"mobilenet_v3_small_100_224\": 224,\n",
        "  \"mobilenet_v3_small_075_224\": 224,\n",
        "  \"mobilenet_v3_large_100_224\": 224,\n",
        "  \"mobilenet_v3_large_075_224\": 224,\n",
        "}\n",
        "\n",
        "model_handle = model_handle_map[model_name]\n",
        "\n",
        "print(f\"Selected model: {model_name} : {model_handle}\")\n",
        "\n",
        "\n",
        "max_dynamic_size = 512\n",
        "if model_name in model_image_size_map:\n",
        "  image_size = model_image_size_map[model_name]\n",
        "  dynamic_size = False\n",
        "  print(f\"Images will be converted to {image_size}x{image_size}\")\n",
        "else:\n",
        "  dynamic_size = True\n",
        "  print(f\"Images will be capped to a max size of {max_dynamic_size}x{max_dynamic_size}\")\n",
        "\n",
        "labels_file = \"https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt\"\n",
        "\n",
        "#download labels and creates a maps\n",
        "downloaded_file = tf.keras.utils.get_file(\"labels.txt\", origin=labels_file)\n",
        "\n",
        "classes = []\n",
        "\n",
        "with open(downloaded_file) as f:\n",
        "  labels = f.readlines()\n",
        "  classes = [l.strip() for l in labels]\n"
      ],
      "metadata": {
        "id": "RaiTFWOD9wz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Select an Input Image\n",
        "\n",
        "image_name = \"banana\" # @param ['tiger', 'bus', 'car', 'cat', 'dog', 'apple', 'banana', 'turtle', 'flamingo', 'piano', 'honeycomb', 'teapot']\n",
        "\n",
        "images_for_test_map = {\n",
        "    \"tiger\": \"https://upload.wikimedia.org/wikipedia/commons/b/b0/Bengal_tiger_%28Panthera_tigris_tigris%29_female_3_crop.jpg\",\n",
        "    #by Charles James Sharp, CC BY-SA 4.0 <https://creativecommons.org/licenses/by-sa/4.0>, via Wikimedia Commons\n",
        "    \"bus\": \"https://upload.wikimedia.org/wikipedia/commons/6/63/LT_471_%28LTZ_1471%29_Arriva_London_New_Routemaster_%2819522859218%29.jpg\",\n",
        "    #by Martin49 from London, England, CC BY 2.0 <https://creativecommons.org/licenses/by/2.0>, via Wikimedia Commons\n",
        "    \"car\": \"https://upload.wikimedia.org/wikipedia/commons/4/49/2013-2016_Toyota_Corolla_%28ZRE172R%29_SX_sedan_%282018-09-17%29_01.jpg\",\n",
        "    #by EurovisionNim, CC BY-SA 4.0 <https://creativecommons.org/licenses/by-sa/4.0>, via Wikimedia Commons\n",
        "    \"cat\": \"https://upload.wikimedia.org/wikipedia/commons/4/4d/Cat_November_2010-1a.jpg\",\n",
        "    #by Alvesgaspar, CC BY-SA 3.0 <https://creativecommons.org/licenses/by-sa/3.0>, via Wikimedia Commons\n",
        "    \"dog\": \"https://upload.wikimedia.org/wikipedia/commons/archive/a/a9/20090914031557%21Saluki_dog_breed.jpg\",\n",
        "    #by Craig Pemberton, CC BY-SA 3.0 <https://creativecommons.org/licenses/by-sa/3.0>, via Wikimedia Commons\n",
        "    \"apple\": \"https://upload.wikimedia.org/wikipedia/commons/1/15/Red_Apple.jpg\",\n",
        "    #by Abhijit Tembhekar from Mumbai, India, CC BY 2.0 <https://creativecommons.org/licenses/by/2.0>, via Wikimedia Commons\n",
        "    \"banana\": \"https://upload.wikimedia.org/wikipedia/commons/1/1c/Bananas_white_background.jpg\",\n",
        "    #by fir0002  flagstaffotos [at] gmail.com\t\tCanon 20D + Tamron 28-75mm f/2.8, GFDL 1.2 <http://www.gnu.org/licenses/old-licenses/fdl-1.2.html>, via Wikimedia Commons\n",
        "    \"turtle\": \"https://upload.wikimedia.org/wikipedia/commons/8/80/Turtle_golfina_escobilla_oaxaca_mexico_claudio_giovenzana_2010.jpg\",\n",
        "    #by Claudio Giovenzana, CC BY-SA 3.0 <https://creativecommons.org/licenses/by-sa/3.0>, via Wikimedia Commons\n",
        "    \"flamingo\": \"https://upload.wikimedia.org/wikipedia/commons/b/b8/James_Flamingos_MC.jpg\",\n",
        "    #by Christian Mehlführer, User:Chmehl, CC BY 3.0 <https://creativecommons.org/licenses/by/3.0>, via Wikimedia Commons\n",
        "    \"piano\": \"https://upload.wikimedia.org/wikipedia/commons/d/da/Steinway_%26_Sons_upright_piano%2C_model_K-132%2C_manufactured_at_Steinway%27s_factory_in_Hamburg%2C_Germany.png\",\n",
        "    #by \"Photo: © Copyright Steinway & Sons\", CC BY-SA 3.0 <https://creativecommons.org/licenses/by-sa/3.0>, via Wikimedia Commons\n",
        "    \"honeycomb\": \"https://upload.wikimedia.org/wikipedia/commons/f/f7/Honey_comb.jpg\",\n",
        "    #by Merdal, CC BY-SA 3.0 <http://creativecommons.org/licenses/by-sa/3.0/>, via Wikimedia Commons\n",
        "    \"teapot\": \"https://upload.wikimedia.org/wikipedia/commons/4/44/Black_tea_pot_cropped.jpg\",\n",
        "    #by Mendhak, CC BY-SA 2.0 <https://creativecommons.org/licenses/by-sa/2.0>, via Wikimedia Commons\n",
        "}\n",
        "\n",
        "img_url = images_for_test_map[image_name]\n",
        "image, original_image = load_image(img_url, image_size, dynamic_size, max_dynamic_size)\n",
        "show_image(image, 'Scaled image')"
      ],
      "metadata": {
        "id": "UMSe6OnJ94oK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run model on image\n",
        "%time probabilities = tf.nn.softmax(classifier(image)).numpy()\n",
        "\n",
        "top_5 = tf.argsort(probabilities, axis=-1, direction=\"DESCENDING\")[0][:5].numpy()\n",
        "np_classes = np.array(classes)\n",
        "\n",
        "# Some models include an additional 'background' class in the predictions, so\n",
        "# we must account for this when reading the class labels.\n",
        "includes_background_class = probabilities.shape[1] == 1001\n",
        "\n",
        "for i, item in enumerate(top_5):\n",
        "  class_index = item if includes_background_class else item + 1\n",
        "  line = f'({i+1}) {class_index:4} - {classes[class_index]}: {probabilities[0][top_5][i]}'\n",
        "  print(line)\n",
        "\n",
        "show_image(image, '')"
      ],
      "metadata": {
        "id": "ojCsYpai99SY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Object detection"
      ],
      "metadata": {
        "id": "KR9eIcnh-AhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's first restart our runtime, since we're going to use a different model (only for class/teaching perposes)\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "KYbN4ldh-BrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instll the dependencies\n",
        "!pip install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt  # install dependencies"
      ],
      "metadata": {
        "id": "5itK4xDu-FnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model\n",
        "import torch\n",
        "\n",
        "# Model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)"
      ],
      "metadata": {
        "id": "XN4FaxHt-Hd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# libs\n",
        "from google.colab import files\n",
        "from matplotlib import pyplot as plt\n",
        "from skimage import io\n",
        "\n",
        "# upload an image - you can try 'citrus.jpg' from /Class10/images\n",
        "uploaded = files.upload()\n",
        "img_path = list(uploaded.keys())[0]\n",
        "img = io.imread(img_path)\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "MeE-Rsuo-Jdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of links to images\n",
        "#imgs = ['https://ultralytics.com/images/zidane.jpg']  # batch of images\n",
        "\n",
        "# Inference\n",
        "results = model(img) # accepts URL, Filename, PIL, OpenCV, Numpy..\n",
        "\n",
        "# Results\n",
        "#results.print()\n",
        "results.show()  # or .save()\n",
        "\n",
        "results.xyxy[0]  # img1 predictions (tensor)\n",
        "results.pandas().xyxy[0]  # img1 predictions (pandas)"
      ],
      "metadata": {
        "id": "jS27y7JH-Lc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if you want, you can load an image to the model from a url, like from a image on a public google drive\n",
        "file_id = '1xDbIZ7oJkdid5c7-NtFAIGEi-zSmaUFL'\n",
        "gdrive_url = f'https://drive.google.com/u/1/uc?id={file_id}&export=download'\n",
        "\n",
        "# Inference\n",
        "results = model(gdrive_url)\n",
        "\n",
        "# Results\n",
        "results.show()"
      ],
      "metadata": {
        "id": "xBotmLj3-OFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if any people were detected in the image\n",
        "if 'person' in results.pandas().xyxy[0]['name'].values:\n",
        "  print('Person detected in image!')\n",
        "else:\n",
        "  print('No people detected in image.')"
      ],
      "metadata": {
        "id": "LAtQDQb_-QNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mask Rcnn\n",
        "\n",
        "[Source](https://colab.research.google.com/drive/1e05Hw_21zl2SgcJ2clE2mWJ1wcoMXUp4)"
      ],
      "metadata": {
        "id": "7InYudeW-dUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "metadata": {
        "id": "tk3SH4eq-jsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's make sure we have a GPU, under device_type\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "metadata": {
        "id": "kh4sOwwt-o1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch rename files script\n",
        "# use it, if you downloaded a lot of images from google, and you want to rename them from img0 to img100 for example\n",
        "\n",
        "# Python 3 code to rename multiple\n",
        "# files in a directory or folder\n",
        "\n",
        "# importing os module\n",
        "import os\n",
        "\n",
        "# Function to rename multiple files\n",
        "def main():\n",
        "\n",
        "\tfolder = \"/content/drive/MyDrive/71254_2023/01_Lectures/Class10/mask_rcnn/dataset/test\"\n",
        "\tfor count, filename in enumerate(os.listdir(folder)):\n",
        "\t\tdst = f\"image{str(count)}.jpg\"\n",
        "\t\tsrc =f\"{folder}/{filename}\" # foldername/filename, if .py file is outside folder\n",
        "\t\tdst =f\"{folder}/{dst}\"\n",
        "\t\t\n",
        "\t\t# rename() function will\n",
        "\t\t# rename all the files\n",
        "\t\tos.rename(src, dst)\n",
        "\n",
        "# Driver Code\n",
        "if __name__ == '__main__':\n",
        "\t\n",
        "\t# Calling main() function\n",
        "\tmain()\n"
      ],
      "metadata": {
        "id": "_8meAizw-ucG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cloning the MASK-RCNN repo\n",
        "!git clone https://github.com/kairess/Mask_RCNN"
      ],
      "metadata": {
        "id": "wz-FuAfD-vMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/deedeeharris/Mask_RCNN"
      ],
      "metadata": {
        "id": "1gvrcOt3-xy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import libs\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import numpy as np\n",
        "import time\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "ROOT_DIR = 'Mask_RCNN'\n",
        "\n",
        "sys.path.append(ROOT_DIR) \n",
        "from mrcnn.config import Config\n",
        "import mrcnn.utils as utils\n",
        "from mrcnn import visualize\n",
        "import mrcnn.model as modellib"
      ],
      "metadata": {
        "id": "WCt6N4AY-1H6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the pretrained model\n",
        "# This will default to sub-directories in your mask_rcnn_dir, but if you want them somewhere else, updated it here.\n",
        "\n",
        "# Directory to save logs and trained model\n",
        "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
        "\n",
        "# Local path to trained weights file\n",
        "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
        "\n",
        "# Download COCO trained weights from Releases if needed\n",
        "if not os.path.exists(COCO_MODEL_PATH):\n",
        "    utils.download_trained_weights(COCO_MODEL_PATH)"
      ],
      "metadata": {
        "id": "5SYx7JHo-2Kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainConfig(Config):\n",
        "    # Give the configuration a recognizable name\n",
        "    NAME = \"custom\"\n",
        "\n",
        "    # Train on 1 GPU and 1 image per GPU. Batch size is 1 (GPUs * images/GPU).\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 5\n",
        "\n",
        "    LEARNING_RATE = 0.001\n",
        "\n",
        "    # Number of classes (including background) - IMPORTANT TO CHANGE ACCORDING TO YOUR LABELS IN YOUR JSON\n",
        "    NUM_CLASSES = 1 + 1  # background + 1 (flowers)\n",
        "\n",
        "    # All of our training images are 1920x1012\n",
        "    IMAGE_MIN_DIM = 512\n",
        "    IMAGE_MAX_DIM = 512\n",
        "    \n",
        "    # Matterport originally used resnet101, but I downsized to fit it on my graphics card\n",
        "    BACKBONE = 'resnet50' # resnet50\n",
        "\n",
        "    # To be honest, I haven't taken the time to figure out what these do\n",
        "    RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)\n",
        "    TRAIN_ROIS_PER_IMAGE = 32\n",
        "    MAX_GT_INSTANCES = 50 \n",
        "    POST_NMS_ROIS_INFERENCE = 500 \n",
        "    POST_NMS_ROIS_TRAINING = 1000 \n",
        "    \n",
        "config = TrainConfig()\n",
        "config.display()"
      ],
      "metadata": {
        "id": "jEoV5_8i-4CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CocoLikeDataset(utils.Dataset):\n",
        "    \"\"\" Generates a COCO-like dataset, i.e. an image dataset annotated in the style of the COCO dataset.\n",
        "        See http://cocodataset.org/#home for more information.\n",
        "    \"\"\"\n",
        "    def load_data(self, annotation_json, images_dir):\n",
        "        \"\"\" Load the coco-like dataset from json\n",
        "        Args:\n",
        "            annotation_json: The path to the coco annotations json file\n",
        "            images_dir: The directory holding the images referred to by the json file\n",
        "        \"\"\"\n",
        "        # Load json from file\n",
        "        json_file = open(annotation_json)\n",
        "        coco_json = json.load(json_file)\n",
        "        json_file.close()\n",
        "        \n",
        "        # Add the class names using the base method from utils.Dataset\n",
        "        source_name = \"coco_like\"\n",
        "        for category in coco_json['categories']:\n",
        "            class_id = category['id']\n",
        "            class_name = category['name']\n",
        "            if class_id < 1:\n",
        "                print('Error: Class id for \"{}\" cannot be less than one. (0 is reserved for the background)'.format(class_name))\n",
        "                return\n",
        "            \n",
        "            self.add_class(source_name, class_id, class_name)\n",
        "        \n",
        "        # Get all annotations\n",
        "        annotations = {}\n",
        "        for annotation in coco_json['annotations']:\n",
        "            image_id = annotation['image_id']\n",
        "            if image_id not in annotations:\n",
        "                annotations[image_id] = []\n",
        "            annotations[image_id].append(annotation)\n",
        "        \n",
        "        # Get all images and add them to the dataset\n",
        "        seen_images = {}\n",
        "        for image in coco_json['images']:\n",
        "            image_id = image['id']\n",
        "            if image_id in seen_images:\n",
        "                print(\"Warning: Skipping duplicate image id: {}\".format(image))\n",
        "            else:\n",
        "                seen_images[image_id] = image\n",
        "                try:\n",
        "                    image_file_name = image['file_name']\n",
        "                    image_width = image['width']\n",
        "                    image_height = image['height']\n",
        "                except KeyError as key:\n",
        "                    print(\"Warning: Skipping image (id: {}) with missing key: {}\".format(image_id, key))\n",
        "                \n",
        "                image_path = os.path.abspath(os.path.join(images_dir, image_file_name))\n",
        "                image_annotations = annotations[image_id]\n",
        "                \n",
        "                # Add the image using the base method from utils.Dataset\n",
        "                self.add_image(\n",
        "                    source=source_name,\n",
        "                    image_id=image_id,\n",
        "                    path=image_path,\n",
        "                    width=image_width,\n",
        "                    height=image_height,\n",
        "                    annotations=image_annotations\n",
        "                )\n",
        "                \n",
        "    def load_mask(self, image_id):\n",
        "        \"\"\" Load instance masks for the given image.\n",
        "        MaskRCNN expects masks in the form of a bitmap [height, width, instances].\n",
        "        Args:\n",
        "            image_id: The id of the image to load masks for\n",
        "        Returns:\n",
        "            masks: A bool array of shape [height, width, instance count] with\n",
        "                one mask per instance.\n",
        "            class_ids: a 1D array of class IDs of the instance masks.\n",
        "        \"\"\"\n",
        "        image_info = self.image_info[image_id]\n",
        "        annotations = image_info['annotations']\n",
        "        instance_masks = []\n",
        "        class_ids = []\n",
        "        \n",
        "        for annotation in annotations:\n",
        "            class_id = annotation['category_id']\n",
        "            mask = Image.new('1', (image_info['width'], image_info['height']))\n",
        "            mask_draw = ImageDraw.ImageDraw(mask, '1')\n",
        "            for segmentation in annotation['segmentation']:\n",
        "                mask_draw.polygon(segmentation, fill=1)\n",
        "                bool_array = np.array(mask) > 0\n",
        "                instance_masks.append(bool_array)\n",
        "                class_ids.append(class_id)\n",
        "\n",
        "        mask = np.dstack(instance_masks)\n",
        "        class_ids = np.array(class_ids, dtype=np.int32)\n",
        "        \n",
        "        return mask, class_ids"
      ],
      "metadata": {
        "id": "a8wJM3pn-6W2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_folder = r'/content/drive/MyDrive/71254_2023/01_Lectures/Class10/mask_rcnn'\n",
        "\n",
        "dataset_train = CocoLikeDataset()\n",
        "dataset_train.load_data(f'{root_folder}/dataset/train.json', f'{root_folder}/dataset/train/')\n",
        "dataset_train.prepare()\n",
        "\n",
        "dataset_val = CocoLikeDataset()\n",
        "dataset_val.load_data(f'{root_folder}/dataset/val.json', f'{root_folder}/dataset/val/')\n",
        "dataset_val.prepare()\n",
        "\n",
        "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
        "\n",
        "print('Train', len(dataset_train.image_ids))\n",
        "print('Validation', len(dataset_val.image_ids))\n",
        "\n",
        "for image_id in image_ids:\n",
        "    image = dataset_train.load_image(image_id)\n",
        "    mask, class_ids = dataset_train.load_mask(image_id)\n",
        "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
      ],
      "metadata": {
        "id": "kv2Vgx6y-8-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = modellib.MaskRCNN(\n",
        "    mode=\"training\",\n",
        "    config=config,\n",
        "    model_dir=MODEL_DIR)\n",
        "\n",
        "model.load_weights(\n",
        "    COCO_MODEL_PATH,\n",
        "    by_name=True,\n",
        "    exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \"mrcnn_bbox\", \"mrcnn_mask\"])"
      ],
      "metadata": {
        "id": "hKQBs7Lw_AxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the head branches\n",
        "# Passing layers=\"heads\" freezes all layers except the head\n",
        "# layers. You can also pass a regular expression to select\n",
        "# which layers to train by name pattern.\n",
        "start_train = time.time()\n",
        "\n",
        "model.train(\n",
        "    dataset_train,\n",
        "    dataset_val, \n",
        "    learning_rate=config.LEARNING_RATE, \n",
        "    epochs=30, \n",
        "    layers='heads')\n",
        "\n",
        "end_train = time.time()\n",
        "minutes = round((end_train - start_train) / 60, 2)\n",
        "\n",
        "print(f'Training took {minutes} minutes')"
      ],
      "metadata": {
        "id": "wwffe_Q2_GKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine tune all layers\n",
        "# Passing layers=\"all\" trains all layers. You can also \n",
        "# pass a regular expression to select which layers to\n",
        "# train by name pattern.\n",
        "start_train = time.time()\n",
        "\n",
        "model.train(\n",
        "    dataset_train,\n",
        "    dataset_val, \n",
        "    learning_rate=config.LEARNING_RATE / 10,\n",
        "    epochs=20, \n",
        "    layers=\"all\")\n",
        "\n",
        "end_train = time.time()\n",
        "minutes = round((end_train - start_train) / 60, 2)\n",
        "\n",
        "print(f'Training took {minutes} minutes')"
      ],
      "metadata": {
        "id": "SFzeITnQ_I34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download the trained model to disk\n",
        "\n",
        "import shutil\n",
        "\n",
        "original = r'/content/Mask_RCNN/logs/custom20221228T1436/mask_rcnn_custom_0030.h5'\n",
        "target = r'/content/drive/MyDrive/71254_2023/01_Lectures/Class10/mask_rcnn/mask_rcnn_custom_0030_28122022_flowers.h5'\n",
        "\n",
        "shutil.copyfile(original, target)"
      ],
      "metadata": {
        "id": "SivHSeKE_Lke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InferenceConfig(TrainConfig):\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "    DETECTION_MIN_CONFIDENCE = 0.65 # CHANGE HERE IF YOU WANT\n",
        "\n",
        "inference_config = InferenceConfig()\n",
        "\n",
        "# Recreate the model in inference mode\n",
        "test_model = modellib.MaskRCNN(\n",
        "    mode=\"inference\", \n",
        "    config=inference_config,\n",
        "    model_dir=MODEL_DIR)\n",
        "\n",
        "model_path = test_model.find_last()\n",
        "print(model_path)\n",
        "\n",
        "test_model.load_weights(model_path, by_name=True)"
      ],
      "metadata": {
        "id": "jDAvRMrA_Oz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import skimage\n",
        "\n",
        "mask_colors = [\n",
        "    (0., 0., 0.), # Background\n",
        "    (1., 0., 0.), # Red\n",
        "    (0., 1., 0.)  # Green\n",
        "]\n",
        "\n",
        "real_test_dir = f'{root_folder}/dataset/test'\n",
        "image_paths = []\n",
        "\n",
        "for filename in os.listdir(real_test_dir):\n",
        "    if os.path.splitext(filename)[1].lower() in ['.png', '.jpg', '.jpeg']:\n",
        "        image_paths.append(os.path.join(real_test_dir, filename))\n",
        "\n",
        "for image_path in image_paths:\n",
        "    img = skimage.io.imread(image_path)\n",
        "    img_arr = np.array(img)\n",
        "\n",
        "    results = test_model.detect([img_arr], verbose=1)\n",
        "    r = results[0]\n",
        "\n",
        "    colors = tuple(np.take(mask_colors, r['class_ids'], axis=0))\n",
        "\n",
        "    visualize.display_instances(img, r['rois'], r['masks'], r['class_ids'], \n",
        "                                dataset_val.class_names, r['scores'], figsize=(16, 8),\n",
        "                                colors=colors)"
      ],
      "metadata": {
        "id": "6cCfNZwo_RGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# libs\n",
        "from google.colab import files\n",
        "from matplotlib import pyplot as plt\n",
        "from skimage import io\n",
        "\n",
        "# upload an image \n",
        "uploaded = files.upload()\n",
        "img_path = list(uploaded.keys())[0]\n",
        "img = io.imread(img_path)\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "8uSLTZBR_T3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# or download from URL using wget\n",
        "!wget https://www.seipasa.com/files/images/img_flor-de-tomate.jpg -O flower.jpg\n",
        "img = io.imread('flower.jpg')\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "QR8rJuuV_XpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import skimage\n",
        "\n",
        "mask_colors = [\n",
        "    (0., 0., 0.), # Background (class 0)\n",
        "    (1., 0., 0.)#, # Flower (class 1)\n",
        "    #(0., 1., 0.)  # Add here, for other classes\n",
        "]\n",
        "\n",
        "\n",
        "img_arr = np.array(img)\n",
        "\n",
        "results = test_model.detect([img_arr], verbose=1)\n",
        "r = results[0]\n",
        "\n",
        "colors = tuple(np.take(mask_colors, r['class_ids'], axis=0))\n",
        "\n",
        "visualize.display_instances(img, r['rois'], r['masks'], r['class_ids'], \n",
        "                            dataset_val.class_names, r['scores'], figsize=(16, 8),\n",
        "                            colors=colors)"
      ],
      "metadata": {
        "id": "k2AFNeEi_Z7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's access the dictionary values\n",
        "masks = r.get(\"masks\")\n",
        "class_ids = r.get(\"class_ids\")\n",
        "scores = r.get(\"scores\")"
      ],
      "metadata": {
        "id": "T0qM044g_co7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# these are the scores (remember, we could of changed the minimum score up top)\n",
        "scores"
      ],
      "metadata": {
        "id": "W1Y2xUNK_f1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# displaying a mask for example\n",
        "plt.imshow(masks[:,:,5])"
      ],
      "metadata": {
        "id": "wC8s3FfO_hnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's try to mask out the backgruond\n",
        "total_mask = masks[:,:,0].copy()\n",
        "for i in range(len(scores)):\n",
        "  total_mask = total_mask + masks[:,:,i]\n",
        "plt.imshow(total_mask)"
      ],
      "metadata": {
        "id": "euc7z3hs_lJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert mask shape from (500,840) to (500,840,3) \n",
        "threeD_mask = np.stack((total_mask, total_mask, total_mask),axis=-1)"
      ],
      "metadata": {
        "id": "HBsLJFZm_lv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display the flowers segmented\n",
        "plt.imshow(threeD_mask*img)"
      ],
      "metadata": {
        "id": "udv5yftv_orI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}